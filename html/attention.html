<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:x="https://www.texmacs.org/2002/extensions" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <head>
    <title>mxm notes</title>
    <meta charset="utf-8" content="TeXmacs 2.1.4" name="generator"></meta>
    <link href="../resources/notes-base.css" type="text/css" rel="stylesheet"></link>
    <link href="./penguin.png" rel="icon"></link>
    <script src="../resources/highlight.pack.js" language="javascript" defer></script>
    <script src="../resources/notes-base.js" language="javascript" defer></script>
  </head>
  <body>
    <div class="toggle" style="display: none">
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
    </div>
    <div class="notes-header">
      <p>
        <img class="image" src="../penguin.png" width="28.116784"></img><span style="margin-left: 2pt"></span><a href="./main.html">[main]</a><em
        class="notes-header-name">Notes on Programming and Others</em>
      </p>
    </div>
    <p>
      <a id="auto-1"></a>
    </p>
    <h1>Understanding Attention Mechanisms<span style="margin-left: 1em"></span></h1>
    <p>
      This article discusses the attention mechanisms both as an extension to
      the encoder-decoder RNN models, and as a fundamental component to the
      Transformer architecture. It serves as a record of my learning process
      and a reference for the future use.
    </p>
    <h2 id="auto-2">Encoder-Decoder Neural Network<span style="margin-left: 1em"></span></h2>
    <p>
      Machine translation can be viewed as constructing conditional language
      models that generate the most probable output in a target language for a
      given input in the other source language. To express this in a
      mathematical way:
    </p>
    <center>
      <img src="attention-1.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0112837465564738em; vertical-align: -1.08879338842975em; height: 2.01970247933884em"></img>
    </center>
    <p>
      Here, x and y denote the input and output sequence respectively, while
      superscripts in angle brackets (<span class="no-breaks"><img src="attention-2.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: 0em; vertical-align: -0.330953168044077em; height: 0.996716253443526em"></img>)</span>
      indicate the position of each word within the sequence. Traditionally, a
      method called statistical machine translation (SMT) was the dominant
      approach (as used in early versions of Google Translate). SMT models are
      trained on large amount of <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpra</a>, and analyzes
      these corpora to identify statistical relationships between words,
      phrases, and sentence structures in different languages.
    </p>
    <p>
      During the 2010s, another method called neural machine translation (NMT)
      rapidly gained popularity following the successful application of RNN
      models to translation tasks. A <a href="https://arxiv.org/abs/1409.3215">2014 paper</a> demonstrated how
      <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long-short-term memory (LSTM)</a> cells could be employed to
      address sequence-to-sequence problems. The idea is to use an encoder
      LSTM to read the input sequence, one timestep at a time, generating a
      fixed-dimensional vector representation, which was then decoded by a
      second LSTM to produce the corresponding output sequence.
    </p>
    <p>
      To better understand this neural network architecture, we will build an
      encoder&ndash;decoder network to solve a relatively simple task:
      converting human-readable date strings into the ISO date format. And
      here are some task samples:
    </p>
    <center>
      <img src="attention-3.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.259504132231405em; margin-right: -0.0112837465564724em; margin-top: -0.0112837465564737em; vertical-align: -1.46263360881543em; height: 3.1622258953168em"></img>
    </center>
    <p>
      The figure below describes our intented architecture:
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-4.png" width="100%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 1. </b><a id="auto-3"></a>A encoder-decoder network
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      The left-side LSTM is the encoder. To better capture information from
      the input sequences, we use a bidirectional RNN for this component. The
      encoder's hidden and cell states form the vector representation of input
      sequence, and they are then passed to the decoder as its initial state.
      The decoder returns all cells' output sequences instead of states from
      the last cell. It then passes its output to a dense layer which uses
      softmax as the activation funciton, to get probabilities of each output
      character. The complete implementation can be found <a href="https://github.com/marsmxm/marsmxm.github.io/blob/main/resources/articles/attention/seq2seq.py">here</a>.
      We will now build this network step by step.
    </p>
    <p>
      First we need to prepare the training dataset. The <a href="https://faker.readthedocs.io/en/master/">faker</a> is
      used here to generate some random dates, which are then formatted in
      random formats:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>def load_date():
    dt = fake.date_object()
    try:
        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US')
        human_readable = human_readable.lower()
        human_readable = human_readable.replace(',','')
        machine_readable = dt.isoformat()
        
    except AttributeError as e:
        return None, None, None

    return human_readable, machine_readable, dt</tt></class></pre>
    </div>
    <p>
      
    </p>
    <p>
      The size of our training dataset is 100,000:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>m = 100000
dataset = load_dataset(m)
dataset[:5]</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>[('saturday june 29 1996', '1996-06-29'),
 ('15 march 1978', '1978-03-15'),
 ('thursday december 28 2023', '2023-12-28'),
 ('wednesday december 31 1980', '1980-12-31'),
 ('apr 5 1995', '1995-04-05')]</tt></class></pre>
    </div>
    <p>
      
    </p>
    <p>
      The next step is to vectorize all the texts in the dataset. Since we are
      translating date strings, we will use character-level vectorization
      rather than word-level vectorization (as commonly used in NLP):
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>import tensorflow as tf

vocab_size = 50
Tx = 30
Ty = 12
sos = '@'
eos = '$'

def custom_standardization(input_string):
    # Lowercase and remove punctuation except '-'
    lowercase = tf.strings.lower(input_string)
    # Remove all punctuation except '-'
    return tf.strings.regex_replace(lowercase, r&quot;[^\w\s-@$]&quot;, &quot;&quot;)

dates_human = [d[0] for d in dataset]
dates_machine = [d[1] for d in dataset]

vec_layer_human = tf.keras.layers.TextVectorization(
    vocab_size, output_sequence_length=Tx, split=&quot;character&quot;, name=&quot;vec_h&quot;,
    standardize=custom_standardization)
vec_layer_machine = tf.keras.layers.TextVectorization(
    vocab_size, output_sequence_length=Ty, split=&quot;character&quot;, name=&quot;vec_m&quot;,
    standardize=custom_standardization)
    
vec_layer_human.adapt(dates_human)
vec_layer_machine.adapt([f&quot;{sos}{s}{eos}&quot; for s in dates_machine])

print(vec_layer_human.get_vocabulary()[:15])
print(vec_layer_machine.get_vocabulary())</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>['', '[UNK]', ' ', '1', '2', 'a', '0', '9', 'e', 'r', 'y', 'u', 'd', 's', 'n']
['', '[UNK]', '-', '0', '1', '2', '@', '$', '9', '7', '8', '3', '4', '5', '6']</tt></class></pre>
    </div>
    <p>
      The empty string (&ldquo;&rdquo;) and<tt class="verbatim"><class style="font-family: Times New Roman"><tt>[UNK]</tt></class></tt>are
      tensorflow's built-in representations for padding and unknown
      characters, repectively. We use a custom <tt class="verbatim">strandardization</tt>
      function here because we need two special characters, <tt class="verbatim">@</tt>
      and <tt class="verbatim">$</tt>, to denote the start and end of the sequences, and
      in the default settings, these special characters are removed by
      tensorflow's <tt class="verbatim">TextVectorization</tt>. 
    </p>
    <p>
      Next we split the whole dataset into training and validation sets:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>train_size = 80000

X_train = tf.constant(dates_human[:train_size])
X_valid = tf.constant(dates_human[train_size:])
X_train_dec = tf.constant([f&quot;{sos}{s}&quot; for s in dates_machine[:train_size]])
X_valid_dec = tf.constant([f&quot;{sos}{s}&quot; for s in dates_machine[train_size:]])
Y_train = vec_layer_machine([f&quot;{s}{eos}&quot; for s in dates_machine[:train_size]])
Y_valid = vec_layer_machine([f&quot;{s}{eos}&quot; for s in dates_machine[train_size:]])</tt></class></pre>
    </div>
    <p>
      
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
  </body>
</html>