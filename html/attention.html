<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:x="https://www.texmacs.org/2002/extensions" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <head>
    <title>mxm notes</title>
    <meta charset="utf-8" content="TeXmacs 2.1.4" name="generator"></meta>
    <link href="../resources/notes-base.css" type="text/css" rel="stylesheet"></link>
    <link href="../resources/blog-icon.png" rel="icon"></link>
    <script src="../resources/highlight.pack.js" language="javascript" defer></script>
    <script src="../resources/notes-base.js" language="javascript" defer></script>
  </head>
  <body>
    <div class="toggle" style="display: none">
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
    </div>
    <div class="notes-header">
      <p>
        <img class="image" src="../penguin.png" width="28.116784"></img><span style="margin-left: 2pt"></span><a href="./main.html">[main]</a><em
        class="notes-header-name">Notes on Programming and Others</em>
      </p>
    </div>
    <p>
      <a id="auto-1"></a>
    </p>
    <h1>Understanding Attention Mechanisms<span style="margin-left: 1em"></span></h1>
    <p>
      This article discusses the attention mechanisms both as an improvement
      to the encoder-decoder RNN models, and as a fundamental component to the
      Transformer architecture. It serves as a record of my learning process
      and a reference for the future use.
    </p>
    <p>
      <strong>Sections:</strong>
    </p>
    <ol>
      <li>
        <p>
          <a href="#section1">Encoder-Decoder Neural Network</a>
        </p>
      </li>
      <li>
        <p>
          <a href="#section2">Attention Mechanisms</a>
        </p>
      </li>
      <li>
        <p>
          Transformers
        </p>
      </li>
    </ol>
    <h2 id="auto-2"><a id="section1"></a>Encoder-Decoder Neural Network<span style="margin-left: 1em"></span></h2>
    <p>
      Machine translation can be viewed as constructing conditional language
      models that generate the most probable output in a target language for a
      given input in the other source language. To express this in a
      mathematical way:
    </p>
    <center>
      <img src="attention-1.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0112837465564738em; vertical-align: -1.08879338842975em; height: 2.01970247933884em"></img>
    </center>
    <p>
      Here, x and y denote the input and output sequence respectively, while
      superscripts in angle brackets (<span class="no-breaks"><img src="attention-2.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: 0em; vertical-align: -0.330953168044077em; height: 0.996716253443526em"></img>)</span>
      indicate the position of each word within the sequence. Traditionally, a
      method called <strong>statistical machine translation</strong> (SMT) was
      the dominant approach (as used in early versions of Google Translate).
      NMT models are trained on large amount of <a href="">parallel corpora</a>,
      and analyzes these corpora to identify statistical relationships between
      words, phrases, and sentence structures in different languages.
    </p>
    <p>
      During the 2010s, another method called <strong>neural machine
      translation</strong> (NMT) rapidly gained popularity following the
      successful application of RNN models to translation tasks. A <a href="https://arxiv.org/abs/1409.3215">2014
      paper</a> demonstrated how <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long-short-term memory (LSTM)</a>
      cells could be employed to address sequence-to-sequence problems. The
      idea is to use an encoder LSTM to read the input sequence, generating a
      fixed-dimensional vector representation, which was then decoded by a
      second LSTM to produce the corresponding output sequence.
    </p>
    <p>
      To better understand this neural network architecture, we will build an
      encoder&ndash;decoder network to solve a relatively simple task:
      converting human-readable date strings into the ISO date format. And
      here are some task samples:
    </p>
    <center>
      <img src="attention-3.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.259504132231405em; margin-right: -0.0112837465564724em; margin-top: -0.0112837465564737em; vertical-align: -2.06981818181818em; height: 4.37661707988981em"></img>
    </center>
    <p>
      The figure below describes our intended architecture:
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-4.png" width="100%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 1. </b><a id="auto-3"></a>A encoder-decoder network
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      The left-side LSTM is the encoder. To better capture information from
      the input sequences, we use a bidirectional RNN for this component. The
      encoder's hidden and cell states form the vector representation of input
      sequence, and they are then passed to the decoder as its initial state.
      The ISO-formatted dates are also used as inputs to the decoder during
      training, but shifted back by one step. In other words, during training
      the decoder is given as input the character that it should have output
      at the previous step, regardless of what it actually output. This is
      called teacher forcing&mdash;a technique that improves the model's
      performance. The decoder returns all cells' output sequences instead of
      states from the last cell. It then passes its output to a dense layer
      which uses softmax as the activation function, to get probabilities of
      each output character. We will now build this network step by step (the
      complete implementation can be found <a href="https://github.com/marsmxm/marsmxm.github.io/blob/main/resources/articles/attention/seq2seq.py">here</a>).
    </p>
    <p>
      First we need to prepare the training dataset. The <a href="https://faker.readthedocs.io/en/master/">faker</a> is
      used here to generate some random dates, which are then formatted in
      random formats:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>def load_date():
    dt = fake.date_object()
    try:
        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US')
        human_readable = human_readable.lower()
        human_readable = human_readable.replace(',','')
        machine_readable = dt.isoformat()
        
    except AttributeError as e:
        return None, None, None

    return human_readable, machine_readable, dt</tt></class></pre>
    </div>
    <p>
      The size of our training dataset is 100,000:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>m = 100000
dataset = load_dataset(m)
dataset[:5]</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>[('saturday june 29 1996', '1996-06-29'),
 ('15 march 1978', '1978-03-15'),
 ('thursday december 28 2023', '2023-12-28'),
 ('wednesday december 31 1980', '1980-12-31'),
 ('apr 5 1995', '1995-04-05')]</tt></class></pre>
    </div>
    <p>
      The next step is to vectorize all the texts in the dataset. Since we are
      translating date strings, we will use character-level vectorization
      rather than word-level vectorization (as commonly used in NLP):
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>import tensorflow as tf

vocab_size = 50
Tx = 30
Ty = 12
sos = '@'
eos = '$'

def custom_standardization(input_string):
    # Lowercase and remove punctuation except '-'
    lowercase = tf.strings.lower(input_string)
    # Remove all punctuation except '-'
    return tf.strings.regex_replace(lowercase, r&quot;[^\w\s-@$]&quot;, &quot;&quot;)

dates_human = [d[0] for d in dataset]
dates_machine = [d[1] for d in dataset]

vec_layer_human = tf.keras.layers.TextVectorization(
    vocab_size, output_sequence_length=Tx, split=&quot;character&quot;, name=&quot;vec_h&quot;,
    standardize=custom_standardization)
vec_layer_machine = tf.keras.layers.TextVectorization(
    vocab_size, output_sequence_length=Ty, split=&quot;character&quot;, name=&quot;vec_m&quot;,
    standardize=custom_standardization)
    
vec_layer_human.adapt(dates_human)
vec_layer_machine.adapt([f&quot;{sos}{s}{eos}&quot; for s in dates_machine])

print(vec_layer_human.get_vocabulary()[:15])
print(vec_layer_machine.get_vocabulary())</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>['', '[UNK]', ' ', '1', '2', 'a', '0', '9', 'e', 'r', 'y', 'u', 'd', 's', 'n']
['', '[UNK]', '-', '0', '1', '2', '@', '$', '9', '7', '8', '3', '4', '5', '6']</tt></class></pre>
    </div>
    <p>
      The empty string (&ldquo;&rdquo;) and <tt class="verbatim"><class style="font-family: Times New Roman"><tt>[UNK]</tt></class></tt>
      are tensorflow's built-in representations for padding and unknown
      characters, respectively. We use a custom <tt class="verbatim">strandardization</tt>
      function here because we need two special characters, <tt class="verbatim">@</tt>
      and <tt class="verbatim">$</tt>, to denote the start and end of the sequences, and
      in the default settings, these special characters are removed by
      tensorflow's <tt class="verbatim">TextVectorization</tt>. 
    </p>
    <p>
      <a id="section1-model"></a>Next we split the whole dataset into training, validation and
      test sets:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>train_size = 60000
valid_size = 20000

X_train = tf.constant(dates_human[:train_size])
X_valid = tf.constant(dates_human[train_size:train_size+valid_size])
X_test = tf.constant(dates_human[train_size+valid_size:])

X_train_dec = tf.constant([f&quot;{sos}{s}&quot; for s in dates_machine[:train_size]])
X_valid_dec = tf.constant(
    [f&quot;{sos}{s}&quot; for s in dates_machine[train_size:train_size+valid_size]])
X_test_dec = tf.constant([f&quot;{sos}{s}&quot; for s in dates_machine[train_size+valid_size:]])

Y_train = vec_layer_machine([f&quot;{s}{eos}&quot; for s in dates_machine[:train_size]])
Y_valid = vec_layer_machine(
    [f&quot;{s}{eos}&quot; for s in dates_machine[train_size:train_size+valid_size]])
Y_test = vec_layer_machine([f&quot;{s}{eos}&quot; for s in dates_machine[train_size+valid_size:]])</tt></class></pre>
    </div>
    <p>
      And we can construct our model now:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>encoder_inputs = tf.keras.layers.Input(name=&quot;encoder_inputs&quot;, shape=[], dtype=tf.string)
decoder_inputs = tf.keras.layers.Input(name=&quot;decoder_inputs&quot;,shape=[], dtype=tf.string)

encoder_input_ids = tf.cast(
    tf.expand_dims(vec_layer_human(encoder_inputs), axis=-1), 
    dtype=tf.float32)
decoder_input_ids = tf.cast(
    tf.expand_dims(vec_layer_machine(decoder_inputs), axis=-1), 
    dtype=tf.float32)

encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_state=True), name=&quot;encoder&quot;)
encoder_outputs, *encoder_states = encoder(encoder_input_ids)
encoder_states = [tf.concat(encoder_states[::2], axis=-1),  # hidden states (0 &amp; 2)
                  tf.concat(encoder_states[1::2], axis=-1)] # cell states (1 &amp; 3)

decoder = tf.keras.layers.LSTM(512, name=&quot;decoder&quot;, return_sequences=True)
decoder_outputs = decoder(decoder_input_ids, initial_state=encoder_states)

output_layer = tf.keras.layers.Dense(vocab_size, name=&quot;dense&quot;, activation=&quot;softmax&quot;)
Y_proba = output_layer(decoder_outputs)

model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],
                       outputs=[Y_proba])
model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;,
              metrics=[&quot;accuracy&quot;])</tt></class></pre>
    </div>
    <p>
      We expand the dimensions of <tt class="verbatim">encoder_input_ids</tt> and <tt
      class="verbatim">decoder_input_ids</tt> because LSTM layers expect a input shape
      of <tt class="verbatim">(batch, timesteps, feature)</tt>, but the outputs of our
      vectorization layers only have two dimensions. Another thing worth
      mentioning is how we compose the <tt class="verbatim">encoder_states</tt>. Since
      we use a bidirectional LSTM as the encoder, we get four states from it
      in total. The first two are hidden and cell states of the forward LSTM,
      and the last two are the corresponding states from the backward one. The
      hidden states are concatenated together, and the cell states are
      concatenated likewise, before being passed to the decoder. We can have a
      look at the model summary now:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>model.summary(line_length=120, expand_nested=True)</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>Model: &quot;model&quot;
________________________________________________________________________________________________________________________
 Layer (type)                       Output Shape                        Param #     Connected to                        
========================================================================================================================
 encoder_inputs (InputLayer)        [(None,)]                           0           []                                  
                                                                                                                        
 vec_h (TextVectorization)          (None, 30)                          0           ['encoder_inputs[0][0]']            
                                                                                                                        
 decoder_inputs (InputLayer)        [(None,)]                           0           []                                  
                                                                                                                        
 tf.expand_dims_2 (TFOpLambda)      (None, 30, 1)                       0           ['vec_h[1][0]']                     
                                                                                                                        
 vec_m (TextVectorization)          (None, 12)                          0           ['decoder_inputs[0][0]']            
                                                                                                                        
 tf.cast (TFOpLambda)               (None, 30, 1)                       0           ['tf.expand_dims_2[0][0]']          
                                                                                                                        
 tf.expand_dims_3 (TFOpLambda)      (None, 12, 1)                       0           ['vec_m[1][0]']                     
                                                                                                                        
 encoder (Bidirectional)            [(None, 512),                       528384      ['tf.cast[0][0]']                   
                                     (None, 256),                                                                       
                                     (None, 256),                                                                       
                                     (None, 256),                                                                       
                                     (None, 256)]                                                                       
                                                                                                                        
 tf.cast_1 (TFOpLambda)             (None, 12, 1)                       0           ['tf.expand_dims_3[0][0]']          
                                                                                                                        
 tf.concat (TFOpLambda)             (None, 512)                         0           ['encoder[0][1]',                   
                                                                                     'encoder[0][3]']                   
                                                                                                                        
 tf.concat_1 (TFOpLambda)           (None, 512)                         0           ['encoder[0][2]',                   
                                                                                     'encoder[0][4]']                   
                                                                                                                        
 decoder (LSTM)                     (None, 12, 512)                     1052672     ['tf.cast_1[0][0]',                 
                                                                                     'tf.concat[0][0]',                 
                                                                                     'tf.concat_1[0][0]']        
       
                                                                                                                        
 dense (Dense)                      (None, 12, 50)                      25650       ['decoder[0][0]']                   
                                                                                                                        
========================================================================================================================
Total params: 1606706 (6.13 MB)
Trainable params: 1606706 (6.13 MB)
Non-trainable params: 0 (0.00 Byte)</tt></class></pre>
    </div>
    <p>
      It indeed has the same topology as we showed earlier in figure 1. The
      training process tasks around 2 minutes on my RTX 3060 card, and the
      evaluation gets a promising result:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>model.fit((X_train, X_train_dec), Y_train, epochs=10,
          validation_data=((X_valid, X_valid_dec), Y_valid))

print(&quot;Evaluate on test data&quot;)
results = model.evaluate((X_test, X_test_dec), Y_test)
print(&quot;test loss, test acc:&quot;, results)</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>test loss, test acc: [0.0018235821044072509, 0.9993454813957214]</tt></class></pre>
    </div>
    <p>
      We can use our model to convert date formats finally, but it's not as
      simple as calling <tt class="verbatim"><class style="font-family: Times New Roman"><tt>model.predict()</tt></class></tt>:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>import numpy as np

def translate(hunman_date):
    translation = &quot;&quot;
    for t in range(Ty):
        X = np.array([hunman_date])  # encoder input 
        X_dec = np.array([sos + translation])  # decoder input
        y_proba = model.predict((X, X_dec), verbose=0)[0, t]  # last token's probas
        char_id = np.argmax(y_proba)
        predicted_char = vec_layer_machine.get_vocabulary()[char_id]
        if predicted_char == eos:
            break
        translation += predicted_char
    return translation.strip()</tt></class></pre>
    </div>
    <p>
      We call the model in a loop because the decoder expects as input the
      character that was predicted at the previous time step. Let's see how
      our model performs:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>for i in range(5):
    human_date= format_date(fake.date_object(),format=random.choice(FORMATS),locale='en_US')
    print(&quot;human: &quot; + human_date)
    print(&quot;machine: &quot; + translate(human_date) + &quot;\n&quot;)</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>human: Friday, December 20, 2019
machine: 2019-12-20

human: 14.02.73
machine: 1973-02-14

human: Jan 10, 2025
machine: 2025-01-10

human: Friday, February 22, 1985
machine: 1985-02-22

human: Sunday, September 12, 1971
machine: 1971-09-12</tt></class></pre>
    </div>
    <p>
      
    </p>
    <h2 id="auto-4"><a id="section2"></a>Attention Mechanisms<span style="margin-left: 1em"></span></h2>
    <p>
      Although this encoder-decoder network performs well on date translation
      tasks, it still has potential limitations when applied to more complex
      NLP domains. The encoder in this network is essentially compressing text
      into fixed-size mathematical representations, so intuitively, as input
      sentences become longer, the rate of information loss tends to increase,
      leading to less accurate translations. This limitation was the key issue
      that <a href="https://arxiv.org/abs/1409.0473">another influential 2014 paper</a> sought to address.
    </p>
    <p>
      The authors proposed an extension to the basic encoder-decoder model.
      The most distinguishing feature is that the new model doesn't attempt to
      encode the entire input text into a single fixed-length vector. Instead,
      it encodes the input into a sequence of vectors, and the decoder
      selectively chooses the most relevant ones based on the current word
      being generated. To get an intuition, take our date translation task as
      an example. When translating the date string of <tt class="verbatim">&quot;Friday,
      December 20, 2019&quot;</tt>, if the decoder is currently generating the
      month part of <tt class="verbatim">&quot;2019-12-20&quot;</tt>, it only needs to
      focus on <tt class="verbatim">December</tt> from the input, so the model assigns
      higher weights to the vector representation of  <tt class="verbatim">December</tt>.
      This is what the attention mechanism means in the context of this new
      approach. Now let's delve into details of each part, and go through key
      mathematical expressions from the paper.
    </p>
    <h3 id="auto-5"><p>
      General Description
    </p><span style="margin-left: 1em"></span></h3>
    <p>
      In the new model architecture, the probability equation for the output
      word at time step <tt class="verbatim">t</tt> is:
    </p>
    <center>
      <img src="attention-5.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0225454545454545em; margin-right: -0.0112837465564741em; margin-top: -0.0139063360881543em; vertical-align: -0.248220385674931em; height: 0.959118457300275em"></img>
    </center>
    <p>
      Here <tt class="verbatim">g</tt> is a non-linear activation function, and <img
      src="attention-6.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823692em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> is decoder's hidden state at <tt class="verbatim">t</tt> step,
      which is further computed by
    </p>
    <center>
      <img src="attention-7.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0225454545454545em; margin-right: -0.0112837465564741em; margin-top: -0.0112837465564738em; vertical-align: -0.248220385674931em; height: 0.959118457300275em"></img>
    </center>
    <p>
      <img src="attention-8.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564739em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> is previous step's hidden state, and <img src="attention-9.png"
      style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823691em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> is a context vector depending on the encoder's hidden
      states. Please note that, unlike the basic encoder-decoder approach,
      each target word is conditioned on a distinct context, which is computed
      as a weighted sum of hidden states from the encoder:
    </p>
    <center>
      <img src="attention-10.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: -0.0112837465564739em; vertical-align: -1.17467768595041em; height: 2.93752066115703em"></img>
    </center>
    <p>
      The weight <img src="attention-11.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564737em; margin-top: -0.0169256198347107em; vertical-align: -0.16169696969697em; height: 0.629994490358127em"></img> is computed by a <tt class="verbatim">softmax</tt>
      function:
    </p>
    <center>
      <img src="attention-12.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0139063360881544em; vertical-align: -1.11898622589532em; height: 2.4748870523416em"></img>
    </center>
    <p>
      where
    </p>
    <center>
      <img src="attention-13.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0423581267217631em; margin-right: -0.0112837465564741em; margin-top: -0.0112837465564738em; vertical-align: -0.239823691460055em; height: 0.930909090909091em"></img>
    </center>
    <p>
      The parameter e is called <strong>energy</strong> and is obtained by
      training a small alignment model that scores how well the input at
      position <img src="attention-14.png" style="margin-left: -0.0112837465564738em; margin-bottom: -2.20385674931135e-5em; margin-right: -0.0112837465564739em; margin-top: -0.0169256198347107em; vertical-align: -0.0112617079889807em; height: 0.810534435261708em"></img> aligns with the output at position
      <span class="no-breaks"><img src="attention-15.png" style="margin-left: -0.0112837465564738em; margin-bottom: -2.20385674931135e-5em; margin-right: -0.0112837465564739em; margin-top: -0.0169256198347106em; vertical-align: -0.0112617079889807em; height: 0.660099173553719em"></img>.</span>
    </p>
    <p>
      The probability <img src="attention-11.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564737em; margin-top: -0.0169256198347107em; vertical-align: -0.16169696969697em; height: 0.629994490358127em"></img> , or its associated energy <span
      class="no-breaks"><img src="attention-16.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564739em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img>,</span> reflects the importance of
      encoder's hidden state <img src="attention-17.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564737em; margin-top: -0.0112837465564738em; vertical-align: -0.16169696969697em; height: 0.883878787878788em"></img> with respect to the
      decoder's previous hidden state <img src="attention-8.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564739em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> in determining
      the next state <img src="attention-6.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823692em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> and generating <span class="no-breaks"><img
      src="attention-18.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0225454545454545em; margin-right: -0.0564187327823691em; margin-top: -0.00526721763085397em; vertical-align: -0.248220385674931em; height: 0.682292011019284em"></img>.</span> Thus, information can be distributed
      throughout the sequence of encoder hidden states and selectively
      retrieved by the decoder. This is a brief graphical illustration from
      the paper:
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-19.png" height="286.7911968" width="223.5284328"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 2. </b><a id="auto-6"></a>The model is generating the word
              <img src="attention-18.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0225454545454545em; margin-right: -0.0564187327823691em; margin-top: -0.00526721763085397em; vertical-align: -0.248220385674931em; height: 0.682292011019284em"></img> given a source sentence (<span class="no-breaks"><img
              src="attention-20.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.101553719008264em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img>).</span>
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      The plus sign in the middle represents the weighted-sum computation for
      the context vector. The bidirectional LSTM below and the forward LSTM
      above are the encoder and decoder respectively. Let's now go deeper into
      implementation details of this neural network.
    </p>
    <h3 id="auto-7">One Step Attention<span style="margin-left: 1em"></span></h3>
    <p>
      The key part of this new model architecture is the attention
      computation, so let's implement it first:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>from tensorflow.keras.layers import RepeatVector, Concatenate, Dense, Dot, Softmax

repeator = RepeatVector(Tx)
concatenator = Concatenate(axis=-1)
densor1 = Dense(10, activation = &quot;tanh&quot;)
densor2 = Dense(1, activation = &quot;relu&quot;)
activator = Softmax(axis=1, name=&quot;attention_weights&quot;)
dotor = Dot(axes = 1)

def one_step_attention(h, s_prev):
    &quot;&quot;&quot;
    Arguments:
    h -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, n_h)
    s_prev -- previous hidden state of the (decoder) LSTM, numpy-array of shape (m, n_s)
    
    Returns:
    context -- context vector, input of the next (decoder) LSTM cell
    &quot;&quot;&quot;
    
    s_prev = repeator(s_prev)           # (m, Tx, n_s)              
    concat = concatenator([h, s_prev])  # (m, Tx, n_h + n_s)
    energies = densor2(densor1(concat)) # (m, Tx, 1)
    alphas = activator(energies)        # (m, Tx, 1)
    context = dotor([alphas, h])        # (m, 1, n_h)
    
    return context</tt></class></pre>
    </div>
    <p>
      This function computes the attention context at decoder time step t. It
      takes two inputs: the complete sequence of encoder hidden states <tt
      class="verbatim">h</tt> and the decoder's previous hidden state <tt class="verbatim">s_prev</tt>.
    </p>
    <p>
      The decoder state <img src="attention-8.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564739em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> is repeated Tx times to match
      the encoder sequence length, enabling computation of alignment scores
      between <img src="attention-8.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564739em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> and each encoder hidden state <span
      class="no-breaks"><img src="attention-17.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564737em; margin-top: -0.0112837465564738em; vertical-align: -0.16169696969697em; height: 0.883878787878788em"></img>.</span> We construct the alignment model
      as a three-layer neural network: two dense layers followed by a softmax
      activation layer.
    </p>
    <p>
      The resulting attention weights (alphas) represent the probability
      distribution indicating how relevant each input word at position t' is
      for generating the current output word at position t. In essence, these
      weights quantify how much attention the decoder should pay to each
      encoder position when producing the current output token.
    </p>
    <p>
      Finally, the attention weights are applied via dot product with the
      encoder hidden states to produce a context vector&mdash;a weighted
      combination of all encoder states that provides tailored information for
      generating each specific output word.
    </p>
    <h3 id="auto-8">Model Building<span style="margin-left: 1em"></span></h3>
    <p>
      This is how our model looks like now:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense

n_h = 64
n_s = 128

def build_model():
    encoder_inputs = Input(name=&quot;encoder_inputs&quot;, shape=[], dtype=tf.string)
    encoder_input_ids = tf.cast(
        tf.expand_dims(vec_layer_human(encoder_inputs), axis=-1), 
        dtype=tf.float32)

    encoder = Bidirectional(LSTM(n_h, return_sequences=True), name=&quot;encoder&quot;)
    encoder_outputs = encoder(encoder_input_ids)

    s0 = Input(shape=(n_s,), name='s0')
    c0 = Input(shape=(n_s,), name='c0')
    s = s0
    c = c0

    decoder_LSTM_cell = LSTM(n_s, name=&quot;decoder&quot;, return_state = True)
    output_layer = Dense(vocab_size, name=&quot;output&quot;, activation=&quot;softmax&quot;)
    outputs = []

    for t in range(Ty):
        context = one_step_attention(encoder_outputs, s)
        _, s, c = decoder_LSTM_cell(inputs=context, initial_state=[s, c])
        out = output_layer(s)
        outputs.append(out)

    # Stack outputs to create a single tensor of shape (batch_size, Ty, vocab_size)
    outputs = tf.stack(outputs, axis=1)

    model = tf.keras.Model(inputs=[encoder_inputs, s0, c0], outputs=outputs)
    model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;,
                metrics=[&quot;accuracy&quot;])

    model.summary(line_length=120, expand_nested=True)
    return model</tt></class></pre>
    </div>
    <p>
      This model differs from the previous architecture in two key aspects:
    </p>
    <ul>
      <li>
        <p>
          Instead of returning only the final hidden state, the encoder now
          outputs the complete sequence of hidden states from all time steps.
          These outputs are essential parts for computing attention weights
          later.
        </p>
      </li>
      <li>
        <p>
          The decoder now implements a manual recurrent structure rather than
          using a standard LSTM layer. At each time step, the decoder cell
          receives two inputs: the attention-weighted context vector (computed
          by the <tt class="verbatim">one_step_attention</tt> function) and the
          hidden/cell states from the previous time step. The cell then passes
          its updated hidden and cell states to the next time step. The
          outputs from all decoder time steps are collected and stacked to
          form the final model output.
        </p>
      </li>
    </ul>
    <p>
      The training and prediction procedures remain unchanged from the
      previous implementation. However, we add a new visualization function to
      analyze the learned attention patterns for test samples.:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>import matplotlib.pyplot as plt

def plot_attention_map(modelx, text):
    layer = modelx.get_layer('attention_weights')
    f = tf.keras.Model(modelx.inputs, [layer.get_output_at(t) for t in range(Ty)])

    s = np.zeros((1, n_s))
    c = np.zeros((1, n_s))
    X = np.array([text])
    attention_weights = f.predict([X, s, c])

    attention_map = np.zeros((Ty, Tx))
    for t in range(Ty):
        for t_prime in range(Tx):
            attention_map[t][t_prime] = attention_weights[t][0, t_prime, 0]

    # Normalize attention map
    row_max = attention_map.max(axis=1)
    attention_map = attention_map / row_max[:, None]

    prediction = modelx.predict([X, s, c], verbose=0)
    predicted_text = []
    for i in range(len(prediction[0])):
        char_id = np.argmax(prediction[0, i]) 
        predicted_char = vec_layer_machine.get_vocabulary()[char_id]

        predicted_text.append(predicted_char)
        
    text_ = list(text)
    
    input_length = len(text)
    output_length = Ty
    
    # Plot the attention_map
    plt.clf()
    f = plt.figure(figsize=(8, 8.5))
    ax = f.add_subplot(1, 1, 1)

    # add image
    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')

    # add colorbar
    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])
    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')
    cbar.ax.set_xlabel('Alpha value (Probability output of the &quot;softmax&quot;)', labelpad=2)

    # add labels
    ax.set_yticks(range(output_length))
    ax.set_yticklabels(predicted_text[:output_length])

    ax.set_xticks(range(input_length))
    ax.set_xticklabels(text_[:input_length], rotation=45)

    ax.set_xlabel('Input Sequence')
    ax.set_ylabel('Output Sequence')

    # add grid and legend
    ax.grid()
    
    return attention_map</tt></class></pre>
    </div>
    <p>
      We extract the <tt class="verbatim">attention_weights</tt> layer from the trained
      model and use it to obtain the attention weights during prediction.
      These weights are then visualized as heat maps, where each cell
      represents the attention score between a specific input position and
      output position. This visualization reveals which parts of the input
      sequence the model focuses on when generating each word in the output,
      providing interpretability into the model's decision-making process.
      Let's try it now:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>plot_attention_map(model, &quot;Monday, February 23, 1998&quot;)</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-21.jpg" width="80%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 3. </b><a id="auto-9"></a>Heat map of Attention Weights 
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      The complete code of this section can be found <a href="https://github.com/marsmxm/marsmxm.github.io/blob/main/resources/articles/attention/attention.py">here</a>.
    </p>
    <h3 id="auto-10">Other Attention Implementations<span style="margin-left: 1em"></span></h3>
    <p>
      The attention mechanism we've implemented is sometimes called
      <strong>additive attention</strong> (due to the concatenation of the
      encoder output with the decoder's previous hidden state) or
      <strong>Bahdanau-style attention</strong> (named after the 2014 paper's
      first author). Another common attention mechanism, known as
      <strong>dot-product attention</strong> or <strong>Luong-style
      attention</strong>, was proposed shortly after, in <a href="https://arxiv.org/abs/1508.04025">a 2015
      paper</a>.
    </p>
    <p>
      Compared to Bahdanau-style attention, the key difference is that, 
      instead of using the decoder's hidden state from the previous time step
      (t-1), Luong attention uses the decoder's current hidden state <img src="attention-22.png"
      style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823692em; margin-top: -0.0112837465564738em; vertical-align: -0.16169696969697em; height: 0.883878787878788em"></img> at step t to: (a) compute the attention weights alongside
      the encoder's hidden states, and (b) be combined with the resulting
      context vector to produce an attentional hidden state <span class="no-breaks"><img
      src="attention-23.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564738em; margin-top: -0.00564187327823695em; vertical-align: -0.16169696969697em; height: 1.12647933884298em"></img>.</span> The attentional hidden state is then fed to
      an activation layer for the final prediciton.
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-24.png" width="60%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 4. </b><a id="auto-11"></a>The global attention model of
              Luong-style attention
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      The <img src="attention-25.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823691em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> in the figure denotes the attention weights
      (<img src="attention-26.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823691em; margin-top: -0.0169256198347107em; vertical-align: -0.16169696969697em; height: 0.629994490358127em"></img> in our previous example). These weights are
      computed by applying a probability distribution to the attention scores
      (or energies in our example), following the same process described
      earlier:
    </p>
    <center>
      <img src="attention-27.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0225674931129478em; vertical-align: -1.76308539944904em; height: 4.02265564738292em"></img>
    </center>
    <p>
      But how these scores are computed is different. There are three
      alternative ways proposed in the paper:
    </p>
    <center>
      <img src="attention-28.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0474710743801652em; margin-right: -0.0112837465564724em; margin-top: -0.0312066115702478em; vertical-align: -1.36841873278237em; height: 3.05908539944904em"></img>
    </center>
    <p>
      The <tt class="verbatim">dot</tt> and <tt class="verbatim">general</tt> equations are two
      dot-product variants, and the third one is the same calculation used for
      Bahdanau-style attention. When the authors compared both dot-product
      approaches against the concatenative attention mechanism, they found
      that the dot-product variants achieved better performance. For this
      reason, dot-product attention is more popular nowadays. 
    </p>
    <p>
      Keras provides a <tt class="verbatim">Attention</tt> layer for dot-product
      attention, we can add it to the  <a href="#section1-model">basic encoder-decoder
      network</a> with some other slight modifications to implement the
      Luong-style attention. 
    </p>
    <p>
      We first need to also set <tt class="verbatim">return_sequences=True</tt> for the
      encoder:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>encoder = tf.keras.layers.Bidirectional(
    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True), name=&quot;encoder&quot;)</tt></class></pre>
    </div>
    <p>
      And then, we create the attention layer and pass it the decoder's states
      and the encoder's outputs. Lastly, we pass the attention layer's outputs
      directly to a activation layer, to get the final prediction:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>attention_layer = tf.keras.layers.Attention()
attention_outputs = attention_layer([decoder_outputs, encoder_outputs])

output_layer = tf.keras.layers.Dense(vocab_size, name=&quot;dense&quot;, activation=&quot;softmax&quot;)
Y_proba = output_layer(attention_outputs)</tt></class></pre>
    </div>
    <p>
      We can map our code to the components in Figure 4. The <tt class="verbatim">decoder_outputs</tt>
      and <tt class="verbatim">encoder_outputs</tt> correspond to <img src="attention-29.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564738em; margin-top: -0.0225674931129477em; vertical-align: -0.16169696969697em; height: 1.04185123966942em"></img>
      and <img src="attention-22.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823692em; margin-top: -0.0112837465564738em; vertical-align: -0.16169696969697em; height: 0.883878787878788em"></img> respectively, <tt class="verbatim">tf.keras.layers.Attention()</tt>
      implements the Attention layer, and <tt class="verbatim">attention_outputs</tt>
      represents <img src="attention-30.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823692em; margin-top: -0.00564187327823695em; vertical-align: -0.16169696969697em; height: 1.12647933884298em"></img> from the figure. 
    </p>
  </body>
</html>