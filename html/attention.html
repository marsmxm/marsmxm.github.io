<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:x="https://www.texmacs.org/2002/extensions" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <head>
    <title>Understanding Attention Mechanisms</title>
    <meta charset="utf-8" content="TeXmacs 2.1.4" name="generator"></meta>
    <link href="../resources/notes-base.css" type="text/css" rel="stylesheet"></link>
    <link href="../resources/blog-icon.png" rel="icon"></link>
    <script src="../resources/highlight.pack.js" language="javascript" defer></script>
    <script src="../resources/notes-base.js" language="javascript" defer></script>
  </head>
  <body>
    <div class="toggle" style="display: none">
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
    </div>
    <div class="notes-header">
      <p>
        <img class="image" src="../penguin.png" width="28.116784"></img><span style="margin-left: 2pt"></span><a href="./main.html">[main]</a><em
        class="notes-header-name">Feel the Water</em>
      </p>
    </div>
    <p>
      <a id="auto-1"></a>
    </p>
    <h1>Understanding Attention Mechanisms<span style="margin-left: 1em"></span></h1>
    <p>
      This article discusses the attention mechanisms both as an improvement
      to the encoder-decoder RNN models, and as a fundamental component to the
      Transformer architecture. It serves as a record of my learning process
      and a reference for the future use.
    </p>
    <p>
      <strong>Sections:</strong>
    </p>
    <ol>
      <li>
        <p>
          <a href="#section1">Encoder-Decoder Neural Network</a>
        </p>
      </li>
      <li>
        <p>
          <a href="#section2">Attention Mechanisms</a>
        </p>
      </li>
      <li>
        <p>
          <a href="#section3">Transformers</a>
        </p>
      </li>
    </ol>
    <h2 id="auto-2"><a id="section1"></a>Encoder-Decoder Neural Network<span style="margin-left: 1em"></span></h2>
    <p>
      Machine translation can be viewed as constructing conditional language
      models that generate the most probable output in a target language for a
      given input in the other source language. To express this in a
      mathematical way:
    </p>
    <center>
      <img src="attention-1.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0112837465564738em; vertical-align: -1.08879338842975em; height: 2.01970247933884em"></img>
    </center>
    <p>
      Here, x and y denote the input and output sequence respectively, while
      superscripts in angle brackets (<span class="no-breaks"><img src="attention-2.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: 0em; vertical-align: -0.330953168044077em; height: 0.996716253443526em"></img>)</span>
      indicate the position of each word within the sequence. Traditionally, a
      method called <strong>statistical machine translation</strong> (SMT) was
      the dominant approach (as used in early versions of Google Translate).
      NMT models are trained on large amount of <a href="">parallel corpora</a>,
      and analyzes these corpora to identify statistical relationships between
      words, phrases, and sentence structures in different languages.
    </p>
    <p>
      During the 2010s, another method called <strong>neural machine
      translation</strong> (NMT) rapidly gained popularity following the
      successful application of RNN models to translation tasks. A <a href="https://arxiv.org/abs/1409.3215">2014
      paper</a> demonstrated how <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long-short-term memory (LSTM)</a>
      cells could be employed to address sequence-to-sequence problems. The
      idea is to use an encoder LSTM to read the input sequence, generating a
      fixed-dimensional vector representation, which was then decoded by a
      second LSTM to produce the corresponding output sequence.
    </p>
    <p>
      To better understand this neural network architecture, we will build an
      encoder&ndash;decoder network to solve a relatively simple task:
      converting human-readable date strings into the ISO date format. And
      here are some task samples:
    </p>
    <center>
      <img src="attention-3.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.259504132231405em; margin-right: -0.0112837465564724em; margin-top: -0.0112837465564737em; vertical-align: -2.06981818181818em; height: 4.37661707988981em"></img>
    </center>
    <p>
      The figure below describes our intended architecture:
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-4.png" width="100%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 1. </b><a id="auto-3"></a>A encoder-decoder network
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      The left-side LSTM is the encoder. To better capture information from
      the input sequences, we use a bidirectional RNN for this component. The
      encoder's hidden and cell states form the vector representation of input
      sequence, and they are then passed to the decoder as its initial state.
      The ISO-formatted dates are also used as inputs to the decoder during
      training, but shifted back by one step. In other words, during training
      the decoder is given as input the character that it should have output
      at the previous step, regardless of what it actually output. This is
      called teacher forcing&mdash;a technique that improves the model's
      performance. The decoder returns all cells' output sequences instead of
      states from the last cell. It then passes its output to a dense layer
      which uses softmax as the activation function, to get probabilities of
      each output character. We will now build this network step by step (the
      complete implementation can be found <a href="https://github.com/marsmxm/marsmxm.github.io/blob/main/resources/articles/attention/seq2seq.py">here</a>).
    </p>
    <p>
      First we need to prepare the training dataset. The <a href="https://faker.readthedocs.io/en/master/">faker</a> is
      used here to generate some random dates, which are then formatted in
      random formats:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
def load_date():
    dt = fake.date_object()
    try:
        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US')
        human_readable = human_readable.lower()
        human_readable = human_readable.replace(',','')
        machine_readable = dt.isoformat()
        
    except AttributeError as e:
        return None, None, None

    return human_readable, machine_readable, dt</pre>
    </div>
    <p>
      The size of our training dataset is 100,000:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
def load_dataset(m):
    dataset = []

    for i in tqdm(range(m)):
        h, m, _ = load_date()
        if h is not None:
            dataset.append((h, m))
 
    return dataset


m = 100000
dataset = load_dataset(m)
dataset[:5]</pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>[('saturday june 29 1996', '1996-06-29'),
 ('15 march 1978', '1978-03-15'),
 ('thursday december 28 2023', '2023-12-28'),
 ('wednesday december 31 1980', '1980-12-31'),
 ('apr 5 1995', '1995-04-05')]</tt></class></pre>
    </div>
    <p>
      The next step is to vectorize all the texts in the dataset. Since we are
      translating date strings, we will use character-level vectorization
      rather than word-level vectorization (as commonly used in NLP):
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
import tensorflow as tf

vocab_size = 50
Tx = 30
Ty = 12
sos = '@'
eos = '$'

def custom_standardization(input_string):
    # Lowercase and remove punctuation except '-'
    lowercase = tf.strings.lower(input_string)
    # Remove all punctuation except '-'
    return tf.strings.regex_replace(lowercase, r&quot;[^\w\s-@$]&quot;, &quot;&quot;)

dates_human = [d[0] for d in dataset]
dates_machine = [d[1] for d in dataset]

vec_layer_human = tf.keras.layers.TextVectorization(
    vocab_size, output_sequence_length=Tx, split=&quot;character&quot;, name=&quot;vec_h&quot;,
    standardize=custom_standardization)
vec_layer_machine = tf.keras.layers.TextVectorization(
    vocab_size, output_sequence_length=Ty, split=&quot;character&quot;, name=&quot;vec_m&quot;,
    standardize=custom_standardization)
    
vec_layer_human.adapt(dates_human)
vec_layer_machine.adapt([f&quot;{sos}{s}{eos}&quot; for s in dates_machine])

print(vec_layer_human.get_vocabulary()[:15])
print(vec_layer_machine.get_vocabulary())</pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
['', '[UNK]', ' ', '1', '2', 'a', '0', '9', 'e', 'r', 'y', 'u', 'd', 's', 'n']
['', '[UNK]', '-', '0', '1', '2', '@', '$', '9', '7', '8', '3', '4', '5', '6']</pre>
    </div>
    <p>
      The empty string (&ldquo;&rdquo;) and <tt class="verbatim"><class style="font-family: Times New Roman"><tt>[UNK]</tt></class></tt>
      are tensorflow's built-in representations for padding and unknown
      characters, respectively. We use a custom <tt class="verbatim">strandardization</tt>
      function here because we need two special characters, <tt class="verbatim">@</tt>
      and <tt class="verbatim">$</tt>, to denote the start and end of the sequences, and
      in the default settings, these special characters are removed by
      tensorflow's <tt class="verbatim">TextVectorization</tt>. 
    </p>
    <p>
      <a id="section1-model"></a>Next we split the whole dataset into training, validation and
      test sets:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
train_size = 60000
valid_size = 20000

X_train = tf.constant(dates_human[:train_size])
X_valid = tf.constant(dates_human[train_size:train_size+valid_size])
X_test = tf.constant(dates_human[train_size+valid_size:])

X_train_dec = tf.constant([f&quot;{sos}{s}&quot; for s in dates_machine[:train_size]])
X_valid_dec = tf.constant(
    [f&quot;{sos}{s}&quot; for s in dates_machine[train_size:train_size+valid_size]])
X_test_dec = tf.constant([f&quot;{sos}{s}&quot; for s in dates_machine[train_size+valid_size:]])

Y_train = vec_layer_machine([f&quot;{s}{eos}&quot; for s in dates_machine[:train_size]])
Y_valid = vec_layer_machine(
    [f&quot;{s}{eos}&quot; for s in dates_machine[train_size:train_size+valid_size]])
Y_test = vec_layer_machine([f&quot;{s}{eos}&quot; for s in dates_machine[train_size+valid_size:]])</pre>
    </div>
    <p>
      And we can construct our model now:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
encoder_inputs = tf.keras.layers.Input(name=&quot;encoder_inputs&quot;, shape=[], dtype=tf.string)
decoder_inputs = tf.keras.layers.Input(name=&quot;decoder_inputs&quot;,shape=[], dtype=tf.string)

encoder_input_ids = tf.cast(
    tf.expand_dims(vec_layer_human(encoder_inputs), axis=-1), 
    dtype=tf.float32)
decoder_input_ids = tf.cast(
    tf.expand_dims(vec_layer_machine(decoder_inputs), axis=-1), 
    dtype=tf.float32)

encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_state=True), name=&quot;encoder&quot;)
encoder_outputs, *encoder_states = encoder(encoder_input_ids)
encoder_states = [tf.concat(encoder_states[::2], axis=-1),  # hidden states (0 &amp; 2)
                  tf.concat(encoder_states[1::2], axis=-1)] # cell states (1 &amp; 3)

decoder = tf.keras.layers.LSTM(512, name=&quot;decoder&quot;, return_sequences=True)
decoder_outputs = decoder(decoder_input_ids, initial_state=encoder_states)

output_layer = tf.keras.layers.Dense(vocab_size, name=&quot;dense&quot;, activation=&quot;softmax&quot;)
Y_proba = output_layer(decoder_outputs)

model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],
                       outputs=[Y_proba])
model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;,
              metrics=[&quot;accuracy&quot;])</pre>
    </div>
    <p>
      We expand the dimensions of <tt class="verbatim">encoder_input_ids</tt> and <tt
      class="verbatim">decoder_input_ids</tt> because LSTM layers expect a input shape
      of <tt class="verbatim">(batch, timesteps, feature)</tt>, but the outputs of our
      vectorization layers only have two dimensions. Another thing worth
      mentioning is how we compose the <tt class="verbatim">encoder_states</tt>. Since
      we use a bidirectional LSTM as the encoder, we get four states from it
      in total. The first two are hidden and cell states of the forward LSTM,
      and the last two are the corresponding states from the backward one. The
      hidden states are concatenated together, and the cell states are
      concatenated likewise, before being passed to the decoder. We can have a
      look at the model summary now:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
model.summary(line_length=120, expand_nested=True)</pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
Model: &quot;model&quot;
________________________________________________________________________________________________________________________
 Layer (type)                       Output Shape                        Param #     Connected to                        
========================================================================================================================
 encoder_inputs (InputLayer)        [(None,)]                           0           []                                  
                                                                                                                        
 vec_h (TextVectorization)          (None, 30)                          0           ['encoder_inputs[0][0]']            
                                                                                                                        
 decoder_inputs (InputLayer)        [(None,)]                           0           []                                  
                                                                                                                        
 tf.expand_dims_2 (TFOpLambda)      (None, 30, 1)                       0           ['vec_h[1][0]']                     
                                                                                                                        
 vec_m (TextVectorization)          (None, 12)                          0           ['decoder_inputs[0][0]']            
                                                                                                                        
 tf.cast (TFOpLambda)               (None, 30, 1)                       0           ['tf.expand_dims_2[0][0]']          
                                                                                                                        
 tf.expand_dims_3 (TFOpLambda)      (None, 12, 1)                       0           ['vec_m[1][0]']                     
                                                                                                                        
 encoder (Bidirectional)            [(None, 512),                       528384      ['tf.cast[0][0]']                   
                                     (None, 256),                                                                       
                                     (None, 256),                                                                       
                                     (None, 256),                                                                       
                                     (None, 256)]                                                                       
                                                                                                                        
 tf.cast_1 (TFOpLambda)             (None, 12, 1)                       0           ['tf.expand_dims_3[0][0]']          
                                                                                                                        
 tf.concat (TFOpLambda)             (None, 512)                         0           ['encoder[0][1]',                   
                                                                                     'encoder[0][3]']                   
                                                                                                                        
 tf.concat_1 (TFOpLambda)           (None, 512)                         0           ['encoder[0][2]',                   
                                                                                     'encoder[0][4]']                   
                                                                                                                        
 decoder (LSTM)                     (None, 12, 512)                     1052672     ['tf.cast_1[0][0]',                 
                                                                                     'tf.concat[0][0]',                 
                                                                                     'tf.concat_1[0][0]']        
       
                                                                                                                        
 dense (Dense)                      (None, 12, 50)                      25650       ['decoder[0][0]']                   
                                                                                                                        
========================================================================================================================
Total params: 1606706 (6.13 MB)
Trainable params: 1606706 (6.13 MB)
Non-trainable params: 0 (0.00 Byte)</pre>
    </div>
    <p>
      It indeed has the same topology as we showed earlier in figure 1. The
      training process tasks around 2 minutes on my RTX 3060 card, and the
      evaluation gets a promising result:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
model.fit((X_train, X_train_dec), Y_train, epochs=10,
          validation_data=((X_valid, X_valid_dec), Y_valid))

print(&quot;Evaluate on test data&quot;)
results = model.evaluate((X_test, X_test_dec), Y_test)
print(&quot;test loss, test acc:&quot;, results)</pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
test loss, test acc: [0.0018235821044072509, 0.9993454813957214]</pre>
    </div>
    <p>
      We can use our model to convert date formats finally, but it's not as
      simple as calling <tt class="verbatim"><class style="font-family: Times New Roman"><tt>model.predict()</tt></class></tt>:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
import numpy as np

def translate(hunman_date):
    translation = &quot;&quot;
    for t in range(Ty):
        X = np.array([hunman_date])  # encoder input 
        X_dec = np.array([sos + translation])  # decoder input
        y_proba = model.predict((X, X_dec), verbose=0)[0, t]  # last token's probas
        char_id = np.argmax(y_proba)
        predicted_char = vec_layer_machine.get_vocabulary()[char_id]
        if predicted_char == eos:
            break
        translation += predicted_char
    return translation.strip()</pre>
    </div>
    <p>
      We call the model in a loop because the decoder expects as input the
      character that was predicted at the previous time step. Let's see how
      our model performs:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
for i in range(5):
    human_date= format_date(fake.date_object(),format=random.choice(FORMATS),locale='en_US')
    print(&quot;human: &quot; + human_date)
    print(&quot;machine: &quot; + translate(human_date) + &quot;\n&quot;)</pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
human: Friday, December 20, 2019
machine: 2019-12-20

human: 14.02.73
machine: 1973-02-14

human: Jan 10, 2025
machine: 2025-01-10

human: Friday, February 22, 1985
machine: 1985-02-22

human: Sunday, September 12, 1971
machine: 1971-09-12</pre>
    </div>
    <p>
      
    </p>
    <h2 id="auto-4"><a id="section2"></a>Attention Mechanisms<span style="margin-left: 1em"></span></h2>
    <p>
      Although this encoder-decoder network performs well on date translation
      tasks, it still has potential limitations when applied to more complex
      NLP domains. The encoder in this network is essentially compressing text
      into fixed-size mathematical representations, so intuitively, as input
      sentences become longer, the rate of information loss tends to increase,
      leading to less accurate translations. This limitation was the key issue
      that <a href="https://arxiv.org/abs/1409.0473">another influential 2014 paper</a> sought to address.
    </p>
    <p>
      The authors proposed an extension to the basic encoder-decoder model.
      The most distinguishing feature is that the new model doesn't attempt to
      encode the entire input text into a single fixed-length vector. Instead,
      it encodes the input into a sequence of vectors, and the decoder
      selectively chooses the most relevant ones based on the current word
      being generated. To get an intuition, take our date translation task as
      an example. When translating the date string of <tt class="verbatim">&quot;Friday,
      December 20, 2019&quot;</tt>, if the decoder is currently generating the
      month part of <tt class="verbatim">&quot;2019-12-20&quot;</tt>, it only needs to
      focus on <tt class="verbatim">December</tt> from the input, so the model assigns
      higher weights to the vector representation of  <tt class="verbatim">December</tt>.
      This is what the attention mechanism means in the context of this new
      approach. Now let's delve into details of each part, and go through key
      mathematical expressions from the paper.
    </p>
    <h3 id="auto-5"><p>
      General Description
    </p><span style="margin-left: 1em"></span></h3>
    <p>
      In the new model architecture, the probability equation for the output
      word at time step <tt class="verbatim">t</tt> is:
    </p>
    <center>
      <img src="attention-5.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0225454545454545em; margin-right: -0.0112837465564741em; margin-top: -0.0139063360881543em; vertical-align: -0.248220385674931em; height: 0.959118457300275em"></img>
    </center>
    <p>
      Here <tt class="verbatim">g</tt> is a non-linear activation function, and <img
      src="attention-6.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823692em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> is decoder's hidden state at <tt class="verbatim">t</tt> step,
      which is further computed by
    </p>
    <center>
      <img src="attention-7.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0225454545454545em; margin-right: -0.0112837465564741em; margin-top: -0.0112837465564738em; vertical-align: -0.248220385674931em; height: 0.959118457300275em"></img>
    </center>
    <p>
      <img src="attention-8.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564739em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> is previous step's hidden state, and <img src="attention-9.png"
      style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823691em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> is a context vector depending on the encoder's hidden
      states. Please note that, unlike the basic encoder-decoder approach,
      each target word is conditioned on a distinct context, which is computed
      as a weighted sum of hidden states from the encoder:
    </p>
    <center>
      <img src="attention-10.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: -0.0112837465564739em; vertical-align: -1.17467768595041em; height: 2.93752066115703em"></img>
    </center>
    <p>
      The weight <img src="attention-11.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564737em; margin-top: -0.0169256198347107em; vertical-align: -0.16169696969697em; height: 0.629994490358127em"></img> is computed by a <tt class="verbatim">softmax</tt>
      function:
    </p>
    <center>
      <img src="attention-12.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0139063360881544em; vertical-align: -1.11898622589532em; height: 2.4748870523416em"></img>
    </center>
    <p>
      where
    </p>
    <center>
      <img src="attention-13.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0423581267217631em; margin-right: -0.0112837465564741em; margin-top: -0.0112837465564738em; vertical-align: -0.239823691460055em; height: 0.930909090909091em"></img>
    </center>
    <p>
      The parameter e is called <strong>energy</strong> and is obtained by
      training a small alignment model that scores how well the input at
      position <img src="attention-14.png" style="margin-left: -0.0112837465564738em; margin-bottom: -2.20385674931135e-5em; margin-right: -0.0112837465564739em; margin-top: -0.0169256198347107em; vertical-align: -0.0112617079889807em; height: 0.810534435261708em"></img> aligns with the output at position
      <span class="no-breaks"><img src="attention-15.png" style="margin-left: -0.0112837465564738em; margin-bottom: -2.20385674931135e-5em; margin-right: -0.0112837465564739em; margin-top: -0.0169256198347106em; vertical-align: -0.0112617079889807em; height: 0.660099173553719em"></img>.</span>
    </p>
    <p>
      The probability <img src="attention-11.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564737em; margin-top: -0.0169256198347107em; vertical-align: -0.16169696969697em; height: 0.629994490358127em"></img> , or its associated energy <span
      class="no-breaks"><img src="attention-16.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564739em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img>,</span> reflects the importance of
      encoder's hidden state <img src="attention-17.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564737em; margin-top: -0.0112837465564738em; vertical-align: -0.16169696969697em; height: 0.883878787878788em"></img> with respect to the
      decoder's previous hidden state <img src="attention-8.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564739em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> in determining
      the next state <img src="attention-6.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823692em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> and generating <span class="no-breaks"><img
      src="attention-18.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0225454545454545em; margin-right: -0.0564187327823691em; margin-top: -0.00526721763085397em; vertical-align: -0.248220385674931em; height: 0.682292011019284em"></img>.</span> Thus, information can be distributed
      throughout the sequence of encoder hidden states and selectively
      retrieved by the decoder. This is a brief graphical illustration from
      the paper:
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-19.png" height="286.7911968" width="223.5284328"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 2. </b><a id="auto-6"></a>The model is generating the word
              <img src="attention-18.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0225454545454545em; margin-right: -0.0564187327823691em; margin-top: -0.00526721763085397em; vertical-align: -0.248220385674931em; height: 0.682292011019284em"></img> given a source sentence (<span class="no-breaks"><img
              src="attention-20.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.101553719008264em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img>).</span>
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      The plus sign in the middle represents the weighted-sum computation for
      the context vector. The bidirectional LSTM below and the forward LSTM
      above are the encoder and decoder respectively. Let's now go deeper into
      implementation details of this neural network.
    </p>
    <h3 id="auto-7">One Step Attention<span style="margin-left: 1em"></span></h3>
    <p>
      The key part of this new model architecture is the attention
      computation, so let's implement it first:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
from tensorflow.keras.layers import RepeatVector, Concatenate, Dense, Dot, Softmax

repeator = RepeatVector(Tx)
concatenator = Concatenate(axis=-1)
densor1 = Dense(10, activation = &quot;tanh&quot;)
densor2 = Dense(1, activation = &quot;relu&quot;)
activator = Softmax(axis=1, name=&quot;attention_weights&quot;)
dotor = Dot(axes = 1)

def one_step_attention(h, s_prev):
    &quot;&quot;&quot;
    Arguments:
    h -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, n_h)
    s_prev -- previous hidden state of the (decoder) LSTM, numpy-array of shape (m, n_s)
    
    Returns:
    context -- context vector, input of the next (decoder) LSTM cell
    &quot;&quot;&quot;
    
    s_prev = repeator(s_prev)           # (m, Tx, n_s)              
    concat = concatenator([h, s_prev])  # (m, Tx, n_h + n_s)
    energies = densor2(densor1(concat)) # (m, Tx, 1)
    alphas = activator(energies)        # (m, Tx, 1)
    context = dotor([alphas, h])        # (m, 1, n_h)
    
    return context</pre>
    </div>
    <p>
      This function computes the attention context at decoder time step t. It
      takes two inputs: the complete sequence of encoder hidden states <tt
      class="verbatim">h</tt> and the decoder's previous hidden state <tt class="verbatim">s_prev</tt>.
    </p>
    <p>
      The decoder state <img src="attention-8.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564739em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> is repeated Tx times to match
      the encoder sequence length, enabling computation of alignment scores
      between <img src="attention-8.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564739em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> and each encoder hidden state <span
      class="no-breaks"><img src="attention-17.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564737em; margin-top: -0.0112837465564738em; vertical-align: -0.16169696969697em; height: 0.883878787878788em"></img>.</span> We construct the alignment model
      as a three-layer neural network: two dense layers followed by a softmax
      activation layer.
    </p>
    <p>
      The resulting attention weights (alphas) represent the probability
      distribution indicating how relevant each input word at position t' is
      for generating the current output word at position t. In essence, these
      weights quantify how much attention the decoder should pay to each
      encoder position when producing the current output token.
    </p>
    <p>
      Finally, the attention weights are applied via dot product with the
      encoder hidden states to produce a context vector&mdash;a weighted
      combination of all encoder states that provides tailored information for
      generating each specific output word.
    </p>
    <h3 id="auto-8">Model Building<span style="margin-left: 1em"></span></h3>
    <p>
      This is how our model looks like now:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense

n_h = 64
n_s = 128

def build_model():
    encoder_inputs = Input(name=&quot;encoder_inputs&quot;, shape=[], dtype=tf.string)
    encoder_input_ids = tf.cast(
        tf.expand_dims(vec_layer_human(encoder_inputs), axis=-1), 
        dtype=tf.float32)

    encoder = Bidirectional(LSTM(n_h, return_sequences=True), name=&quot;encoder&quot;)
    encoder_outputs = encoder(encoder_input_ids)

    s0 = Input(shape=(n_s,), name='s0')
    c0 = Input(shape=(n_s,), name='c0')
    s = s0
    c = c0

    decoder_LSTM_cell = LSTM(n_s, name=&quot;decoder&quot;, return_state = True)
    output_layer = Dense(vocab_size, name=&quot;output&quot;, activation=&quot;softmax&quot;)
    outputs = []

    for t in range(Ty):
        context = one_step_attention(encoder_outputs, s)
        _, s, c = decoder_LSTM_cell(inputs=context, initial_state=[s, c])
        out = output_layer(s)
        outputs.append(out)

    # Stack outputs to create a single tensor of shape (batch_size, Ty, vocab_size)
    outputs = tf.stack(outputs, axis=1)

    model = tf.keras.Model(inputs=[encoder_inputs, s0, c0], outputs=outputs)
    model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;,
                metrics=[&quot;accuracy&quot;])

    model.summary(line_length=120, expand_nested=True)
    return model</pre>
    </div>
    <p>
      This model differs from the previous architecture in two key aspects:
    </p>
    <ul>
      <li>
        <p>
          Instead of returning only the final hidden state, the encoder now
          outputs the complete sequence of hidden states from all time steps.
          These outputs are essential parts for computing attention weights
          later.
        </p>
      </li>
      <li>
        <p>
          The decoder now implements a manual recurrent structure rather than
          using a standard LSTM layer. At each time step, the decoder cell
          receives two inputs: the attention-weighted context vector (computed
          by the <tt class="verbatim">one_step_attention</tt> function) and the
          hidden/cell states from the previous time step. The cell then passes
          its updated hidden and cell states to the next time step. The
          outputs from all decoder time steps are collected and stacked to
          form the final model output.
        </p>
      </li>
    </ul>
    <p>
      The training and prediction procedures remain unchanged from the
      previous implementation. However, we add a new visualization function to
      analyze the learned attention patterns for test samples.:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
import matplotlib.pyplot as plt

def plot_attention_map(modelx, text):
    layer = modelx.get_layer('attention_weights')
    f = tf.keras.Model(modelx.inputs, [layer.get_output_at(t) for t in range(Ty)])

    s = np.zeros((1, n_s))
    c = np.zeros((1, n_s))
    X = np.array([text])
    attention_weights = f.predict([X, s, c])

    attention_map = np.zeros((Ty, Tx))
    for t in range(Ty):
        for t_prime in range(Tx):
            attention_map[t][t_prime] = attention_weights[t][0, t_prime, 0]

    # Normalize attention map
    row_max = attention_map.max(axis=1)
    attention_map = attention_map / row_max[:, None]

    prediction = modelx.predict([X, s, c], verbose=0)
    predicted_text = []
    for i in range(len(prediction[0])):
        char_id = np.argmax(prediction[0, i]) 
        predicted_char = vec_layer_machine.get_vocabulary()[char_id]

        predicted_text.append(predicted_char)
        
    text_ = list(text)
    
    input_length = len(text)
    output_length = Ty
    
    # Plot the attention_map
    plt.clf()
    f = plt.figure(figsize=(8, 8.5))
    ax = f.add_subplot(1, 1, 1)

    # add image
    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')

    # add colorbar
    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])
    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')
    cbar.ax.set_xlabel('Alpha value (Probability output of the &quot;softmax&quot;)', labelpad=2)

    # add labels
    ax.set_yticks(range(output_length))
    ax.set_yticklabels(predicted_text[:output_length])

    ax.set_xticks(range(input_length))
    ax.set_xticklabels(text_[:input_length], rotation=45)

    ax.set_xlabel('Input Sequence')
    ax.set_ylabel('Output Sequence')

    # add grid and legend
    ax.grid()
    
    return attention_map</pre>
    </div>
    <p>
      We extract the <tt class="verbatim">attention_weights</tt> layer from the trained
      model and use it to obtain the attention weights during prediction.
      These weights are then visualized as heat maps, where each cell
      represents the attention score between a specific input position and
      output position. This visualization reveals which parts of the input
      sequence the model focuses on when generating each word in the output,
      providing interpretability into the model's decision-making process.
      Let's try it now:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>plot_attention_map(model, &quot;Monday, February 23, 1998&quot;)</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-21.jpg" width="80%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 3. </b><a id="auto-9"></a>Heat map of Attention Weights 
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      The complete code of this section can be found <a href="https://github.com/marsmxm/marsmxm.github.io/blob/main/resources/articles/attention/attention.py">here</a>.
    </p>
    <h3 id="auto-10">Other Attention Implementations<span style="margin-left: 1em"></span></h3>
    <p>
      The attention mechanism we've implemented is sometimes called
      <strong>additive attention</strong> (due to the concatenation of the
      encoder output with the decoder's previous hidden state) or
      <strong>Bahdanau-style attention</strong> (named after the 2014 paper's
      first author). Another common attention mechanism, known as
      <strong>dot-product attention</strong> or <strong>Luong-style
      attention</strong>, was proposed shortly after, in <a href="https://arxiv.org/abs/1508.04025">a 2015
      paper</a>.
    </p>
    <p>
      Compared to Bahdanau-style attention, the key difference is that, 
      instead of using the decoder's hidden state from the previous time step
      (t-1), Luong attention uses the decoder's current hidden state <img src="attention-22.png"
      style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823692em; margin-top: -0.0112837465564738em; vertical-align: -0.16169696969697em; height: 0.883878787878788em"></img> at step t to: (a) compute the attention weights alongside
      the encoder's hidden states, and (b) be combined with the resulting
      context vector to produce an attentional hidden state <span class="no-breaks"><img
      src="attention-23.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564738em; margin-top: -0.00564187327823695em; vertical-align: -0.16169696969697em; height: 1.12647933884298em"></img>.</span> The attentional hidden state is then fed to
      an activation layer for the final prediction. There can be illustrated
      in a figure from their paper:
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-24.png" width="60%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 4. </b><a id="auto-11"></a>The global attention model of
              Luong-style attention
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      The <img src="attention-25.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823691em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> in the figure denotes the attention weights
      (<img src="attention-26.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823691em; margin-top: -0.0169256198347107em; vertical-align: -0.16169696969697em; height: 0.629994490358127em"></img> in our previous example). These weights are
      computed by applying a probability distribution to the attention scores
      (or energies in our example), following the same process described
      earlier:
    </p>
    <center>
      <img src="attention-27.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0225674931129478em; vertical-align: -1.76308539944904em; height: 4.02265564738292em"></img>
    </center>
    <p>
      But how these scores are computed is different. There are three
      alternative ways proposed in the paper:
    </p>
    <center>
      <img src="attention-28.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0474710743801652em; margin-right: -0.0112837465564724em; margin-top: -0.0312066115702478em; vertical-align: -1.36841873278237em; height: 3.05908539944904em"></img>
    </center>
    <p>
      The <tt class="verbatim">dot</tt> and <tt class="verbatim">general</tt> equations are two
      dot-product variants, and the third one is the same calculation used for
      Bahdanau-style attention. When the authors compared both dot-product
      approaches against the concatenate attention mechanism, they found that
      the dot-product variants achieved better performance. For this reason,
      dot-product attention is more popular nowadays. 
    </p>
    <p>
      Keras provides an <tt class="verbatim">Attention</tt> layer for dot-product
      attention, we can add it to the  <a href="#section1-model">basic encoder-decoder
      network</a> with some other slight modifications to implement the
      Luong-style attention. 
    </p>
    <p>
      We first need to also set <tt class="verbatim">return_sequences=True</tt> for the
      encoder:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
encoder = tf.keras.layers.Bidirectional(
    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True), name=&quot;encoder&quot;)</pre>
    </div>
    <p>
      And then, we create the attention layer and pass it the decoder's states
      and the encoder's outputs. Lastly, we pass the attention layer's outputs
      directly to an activation layer, to get the final prediction:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
attention_layer = tf.keras.layers.Attention()
attention_outputs = attention_layer([decoder_outputs, encoder_outputs])

output_layer = tf.keras.layers.Dense(vocab_size, name=&quot;dense&quot;, activation=&quot;softmax&quot;)
Y_proba = output_layer(attention_outputs)</pre>
    </div>
    <p>
      We can map our code to the components in Figure 4. The <tt class="verbatim">decoder_outputs</tt>
      and <tt class="verbatim">encoder_outputs</tt> correspond to <img src="attention-29.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564738em; margin-top: -0.0225674931129477em; vertical-align: -0.16169696969697em; height: 1.04185123966942em"></img>
      and <img src="attention-22.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823692em; margin-top: -0.0112837465564738em; vertical-align: -0.16169696969697em; height: 0.883878787878788em"></img> respectively, <tt class="verbatim">tf.keras.layers.Attention()</tt>
      implements the Attention layer, and <tt class="verbatim">attention_outputs</tt>
      represents <img src="attention-30.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823692em; margin-top: -0.00564187327823695em; vertical-align: -0.16169696969697em; height: 1.12647933884298em"></img> from the figure.
    </p>
    <h3 id="auto-12">Attention Mechanism as a Learnable Information-Retrieval
    System<span style="margin-left: 1em"></span></h3>
    <p>
      An interesting interpretation of the attention mechanism is to think of
      it as a learnable database system. For example, let's say the encoder
      has processed the date string &ldquo;Monday, February 23, 1998&rdquo;,
      and manages to understand that the semantics of this input is:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
{
  &quot;weekday&quot;: &quot;Monday&quot;,
  &quot;month&quot;:   &quot;February&quot;,
  &quot;day&quot;:     &quot;23&quot;,
  &quot;year&quot;:    &quot;1998&quot;
}</pre>
    </div>
    <p>
      So it encodes this information in its vector output. Now suppose the
      decoder has already generated the year part of &ldquo;1998-02-23&rdquo;
      and determines that the month comes next. So it needs to retrieve the
      month value from the encoder's output&mdash;just like a database query
      or a dictionary lookup: <tt class="verbatim">encoded_vector[&quot;month&quot;]</tt>.
      In our earlier examples, the decoder's hidden states (either <img src="attention-8.png"
      style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564739em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img> or <span class="no-breaks"><img src="attention-6.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0564187327823692em; margin-top: -0.00526721763085397em; vertical-align: -0.16169696969697em; height: 0.607052341597796em"></img>)</span> serve as
      <em>queries</em> to retrieve relevant information from the encoder's
      outputs. And the attention layer can be considered as a trained database
      that learns to return the right <em>values</em> for a given query, based
      on the training examples. The &ldquo;lookup&rdquo; process works by
      first computing similarity scores between the decoder's and encoder's
      hidden states, and then using those scores to weight and extract the
      most relevant parts of the encoder's outputs. That's why, in the Keras
      Attention layer documentation, the call arguments are named
      <code>query</code>, <code>value</code> and <tt class="verbatim">key</tt> (<tt
      class="verbatim">key</tt> is usually the same with <tt class="verbatim">value</tt>).
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-31.png" width="100%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 5. </b><a id="auto-13"></a>Call inputs of <tt class="verbatim">keras.layers.Attention</tt>
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      A special use case of the attention mechanism is when the same tensor is
      used as both the query and the value&mdash;this is called
      <em>self-attention</em>. Self-attention is one of the core ideas behind
      our next topic: the Transformer architecture.
    </p>
    <h2 id="auto-14"><a id="section3"></a>Transformers<span style="margin-left: 1em"></span></h2>
    <p>
      The model architectures we've discussed so far are all built on
      recurrent neural network, specifically LSTM cells. The term
      <em>recurrent</em> refers to the fact that the same neural cell is
      applied repeatedly across the sequence: at each step it takes in the
      hidden state from the previous step, updates it with the current input,
      and produces a new hidden state for the next step. This process
      continues until the entire sequence has been processed. It's very much
      like calling a recursive function, with each call frame depending on the
      return value from the inner frame, until the argument eventually reduces
      to its base case. This inherently sequential nature of RNN prevents
      parallelization within a single training example, resulting in a time
      complexity of <img src="attention-32.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0423581267217631em; margin-right: -0.0112837465564737em; margin-top: -0.0139063360881543em; vertical-align: -0.239845730027548em; height: 0.930909090909091em"></img> with respect to the sequence
      length T. As T grows, training time increases linearly, while memory
      constraints also further restrict the batching degree across training
      examples.
    </p>
    <p>
      To overcome these limitations, a team of Google researchers introduced
      the <strong>Transformer</strong> architecture in their 2017 paper
      <em>&ldquo;Attention Is All You Need&rdquo;</em>, which removes
      recurrence entirely and instead relies exclusively on attention
      mechanisms to capture global dependencies between inputs and outputs.
      This design increases the potential for parallelization, allowing the
      training process to scale more efficiently. It also enables the model to
      capture long-range dependencies more effectively than recurrent
      architectures. The transformer architecture is presented below in the
      figure from their paper.
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-33.png" width="60%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 6. </b><a id="auto-15"></a>The Transformer Architecture
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <h3 id="auto-16">Encoder and Decoder Stacks<span style="margin-left: 1em"></span></h3>
    <p>
      This architecture is composed of an encoder (on the left) and a decoder
      (on the right) as well. The encoder consists of <em>N</em> identical
      layers (with <em>N = 6</em> in the original paper), which we will refer
      to as encoder layers. Each encoder layer is further divided into two
      sub-layers: the first is a multi-head self-attention mechanism (more
      details later), and the second is a fully connected feed-forward
      network. Each sub-layer is wrapped with a residual connection from its
      input and followed by layer normalization. These two parts are combined
      as &ldquo;Add &amp; Norm&rdquo; in the figure because they can be
      mathematically expressed as:
    </p>
    <center>
      <img src="attention-34.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0141487603305785em; margin-right: -0.0112837465564741em; margin-top: -0.0112837465564738em; vertical-align: -0.239823691460055em; height: 0.959118457300275em"></img>
    </center>
    <p>
      These &ldquo;Add &amp; Norm&rdquo; operations help stabilize training by
      preventing vanishing gradients and maintaining optimization efficiency
      throughout the network.
    </p>
    <p>
      The structure of the decoder is almost the same, with two key
      differences:
    </p>
    <ul>
      <li>
        <p>
          A <strong>look-ahead mask</strong> is applied to the decoder's
          self-attention layer. Since the entire target sequence (the label
          outputs) is fed into the decoder at once during training, future
          positions beyond the current step would otherwise be visible. To
          prevent the model from &ldquo;cheating&rdquo; by accessing those
          future tokens, the mask blocks attention to positions after the
          current step.
        </p>
      </li>
      <li>
        <p>
          The decoder includes an additional third sub-layer between the
          self-attention and feed-forward components. This intermediate layer
          performs <strong>multi-head cross-attention</strong> over the
          encoder stack's output, enabling the decoder to attend to relevant
          parts of the input sequence during generation.
        </p>
      </li>
    </ul>
    <h3 id="auto-17">Positional Encoding<span style="margin-left: 1em"></span></h3>
    <p>
      Positional encoding layers sit between the embedding layers and the
      encoder/decoder stacks, injecting positional information into the
      embedding vectors. Unlike RNNs, which inherently capture word order
      through sequential processing, the Transformer processes the entire
      input sequence in parallel, making explicit positional encoding
      essential for the model to understand sequence order. The positional
      encodings have the same dimension as the embeddings, so the two can be
      added together. The paper defines these encodings using the following
      formulas:
    </p>
    <center>
      <img src="attention-35.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889805em; margin-right: -0.0112837465564724em; margin-top: -0.0373553719008268em; vertical-align: -2.58133333333333em; height: 5.67396143250689em"></img>
    </center>
    <ul>
      <li>
        <p>
          <img src="attention-36.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0225454545454545em; margin-right: -0.0112837465564739em; margin-top: -0.0169256198347107em; vertical-align: -0.248220385674931em; height: 0.705234159779614em"></img> is the position of the word.
        </p>
      </li>
      <li>
        <p>
          <img src="attention-37.png" style="margin-left: -0.0112837465564738em; margin-bottom: -2.20385674931135e-5em; margin-right: -0.0564187327823692em; margin-top: -0.0112837465564738em; vertical-align: -0.0112617079889807em; height: 0.744727272727273em"></img> is the dimension of the word embedding.
        </p>
      </li>
      <li>
        <p>
          <img src="attention-38.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564737em; margin-top: -0.0112837465564738em; vertical-align: -0.0676804407713499em; height: 0.789862258953168em"></img> where <img src="attention-39.png" style="margin-left: -0.0112837465564738em; margin-bottom: -2.20385674931135e-5em; margin-right: -0.0282093663911845em; margin-top: -0.0112837465564738em; vertical-align: -0.0112617079889807em; height: 0.744727272727273em"></img> is the index
          of the word embedding.
        </p>
      </li>
    </ul>
    <p>
      The positional values of even and odd indices are computed by sine and
      cosine functions respectively, but the angle for each <tt class="verbatim">i</tt>
      is identical. When implementing the positional encoding later, we'll
      observe some interesting properties that help explain why it is defined
      in this particular way.
    </p>
    <h3 id="auto-18">Attention<span style="margin-left: 1em"></span></h3>
    <p>
      The authors refer to their attention mechanism as <strong>Scaled
      Dot-Product Attention</strong>. It's quite similar to the dot-product
      attention we discussed in the previous section, with one key difference:
      the dot-product results are scaled down before being passed to the
      softmax function:
    </p>
    <center>
      <img src="attention-40.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0805289256198347em; margin-right: -0.141046831955922em; margin-top: -0.0373553719008264em; vertical-align: -1.12114600550964em; height: 2.65833608815427em"></img>
    </center>
    <ul>
      <li>
        <p>
          Q, K and V are matrices representing queries, keys and values.
        </p>
      </li>
      <li>
        <p>
          In self-attentions, Q, K, and V are all the same output of the
          previous encoder (or decoder) layer.
        </p>
      </li>
      <li>
        <p>
          In cross-attentions, Q comes from the previous decoder layer, K and
          V come from the output of the encoder.
        </p>
      </li>
      <li>
        <p>
          The the dot-product result is scaled by <span class="no-breaks"><img src="attention-41.png"
          style="margin-left: -0.0112837465564738em; margin-bottom: 0.0693994490358126em; margin-right: -0.0112837465564739em; margin-top: -0.0169256198347107em; vertical-align: -0.79235261707989em; height: 1.76852892561983em"></img>.</span> Without this scaling, large dot-product values
          push the softmax function into saturation regions where gradients
          become extremely small, hindering the learning progress.
        </p>
      </li>
    </ul>
    <p>
      The term <strong>multi-head</strong> refers to computing not just one,
      but <img src="attention-42.png" style="margin-left: -0.0112837465564738em; margin-bottom: -2.20385674931135e-5em; margin-right: -0.0225674931129477em; margin-top: -0.0112837465564738em; vertical-align: -0.0112617079889807em; height: 0.744727272727273em"></img> parallel attentions (or head). Instead of
      applying attention directly to the full-dimensional queries, keys, and
      values, each head uses its own learned linear projections of Q, K, and
      V. The outputs from all heads are then concatenated and passed through a
      final linear layer to produce the result, as illustrated in the figure
      below.
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-43.png" width="50%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 7. </b><a id="auto-19"></a>Multi-Head Attention
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      We can think of multi-head attentions as applying the attention
      mechanisms several times to detect different features. We will next
      implement all the key components, and connect them together to form the
      whole Transformer network.
    </p>
    <h3 id="auto-20">Transformer Implementation<span style="margin-left: 1em"></span></h3>
    <p>
      Let's first work on the positional encodings. The table below shows
      intuitively the resulting encoding matrix we want for each input
      sequence:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; padding-left: 0em; text-align: center"><p>
          
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          k = 0
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          k = 1
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          <span style="margin-left: 0.16665em"></span>.<span style="margin-left: 0.16665em"></span>.<span style="margin-left: 0.16665em"></span>.<span
          style="margin-left: 0.16665em"></span>
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          k = d - 2
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; padding-right: 0em; text-align: center"><p>
          k = d - 1
        </p></td>
      </tr><tr>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; padding-left: 0em; text-align: center"><p>
          encoding(0)
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          <img src="attention-44.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: -0.0373553719008266em; vertical-align: -1.11883195592286em; height: 2.55182369146006em"></img>
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          <img src="attention-45.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0373553719008266em; vertical-align: -1.11883195592286em; height: 2.55182369146006em"></img>
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          <img src="attention-46.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: -0.0373553719008264em; vertical-align: -1.16396694214876em; height: 2.574391184573em"></img>
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; padding-right: 0em; text-align: center"><p>
          <img src="attention-47.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0373553719008264em; vertical-align: -1.16396694214876em; height: 2.574391184573em"></img>
        </p></td>
      </tr><tr>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; padding-left: 0em; text-align: center"><p>
          encoding(1)
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          <img src="attention-48.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: -0.0373553719008266em; vertical-align: -1.11883195592286em; height: 2.55182369146006em"></img>
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          <img src="attention-49.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0373553719008266em; vertical-align: -1.11883195592286em; height: 2.55182369146006em"></img>
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          <img src="attention-50.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: -0.0373553719008264em; vertical-align: -1.16396694214876em; height: 2.574391184573em"></img>
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; padding-right: 0em; text-align: center"><p>
          <img src="attention-51.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0373553719008264em; vertical-align: -1.16396694214876em; height: 2.574391184573em"></img>
        </p></td>
      </tr><tr>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; padding-left: 0em; text-align: center"><p>
          <span style="margin-left: 0.16665em"></span>.<span style="margin-left: 0.16665em"></span>.<span style="margin-left: 0.16665em"></span>.<span
          style="margin-left: 0.16665em"></span>
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; padding-right: 0em; text-align: center"><p>
          
        </p></td>
      </tr><tr>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; padding-left: 0em; text-align: center"><p>
          encoding(pos)
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          <img src="attention-52.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: -0.0373553719008264em; vertical-align: -1.11883195592286em; height: 2.40004407713499em"></img>
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          <img src="attention-53.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0373553719008264em; vertical-align: -1.11883195592286em; height: 2.40004407713499em"></img>
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; text-align: center"><p>
          <img src="attention-54.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: -0.0373553719008264em; vertical-align: -1.16396694214876em; height: 2.51797245179063em"></img>
        </p></td>
        <td style="width: 16.6666666666667%; border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; padding-right: 0em; text-align: center"><p>
          <img src="attention-55.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0373553719008264em; vertical-align: -1.16396694214876em; height: 2.51797245179063em"></img>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      The functions below calculate the positional encoding matrix:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
def get_angles(pos, k, d):
  &quot;&quot;&quot;
  Arguments:
  pos -- column vector [[0], [1], &hellip;,[N-1]]
  k   -- row vector [[0, 1, 2, &hellip;, d-1]]
  d   -- int, size of the embedding dimension

  Returns:
  angles -- (pos, d) matrix
  &quot;&quot;&quot;

  i = k // 2
  angles = pos / (10000 ** (2 * i / d))

  return angles


def positional_encoding(positions, d):
  &quot;&quot;&quot;
  Arguments:
  positions -- int, number of positions to be encoded 
  d         -- int, size of embedding dimension

  Returns:
  pos_encoding -- (1, pos, d) A matrix with the positional encodings
  &quot;&quot;&quot;

  angles = get_angles(np.arange(positions)[:, np.newaxis],
                      np.arange(d)[np.newaxis, :],
                      d)

  # apply sin to even indices: 2i
  angles[:, 0::2] = np.sin(angles[:, 0::2])
  # apply cos to odd indices: 2i+1
  angles[:, 1::2] = np.cos(angles[:, 1::2])

  pos_encoding = angles[np.newaxis, &hellip;]

  return tf.cast(pos_encoding, dtype=tf.float32)</pre>
    </div>
    <p>
      The helper function <tt class="verbatim">get_angles</tt> assumes that <tt class="verbatim">pos</tt>
      is a column vector and <tt class="verbatim">k</tt> is a row vector. When the
      division operator is applied, broadcasting first expands <tt class="verbatim">k</tt>
      across each row of <tt class="verbatim">pos</tt>. Then, each scalar element of <tt
      class="verbatim">pos</tt> is broadcast across the replicated rows of <tt class="verbatim">k</tt>,
      producing the matrix structure shown above. The
      <code>positional_function</code> accepts two integer inputs: the number
      of positions and the size of the embedding dimension. It uses
      <code>np.newaxis</code> to reshape <code>pos</code> and <code>k</code>
      into column and row vectors before passing them to
      <code>get_angles</code>. The function then applies <em>sine</em> to
      even-indexed columns and <em>cosine</em> to odd-indexed columns.
      Finally, it expands the matrix dimensions at axis 0 to match the
      expected shape of <tt class="verbatim">(1, pos, d)</tt>. This allows the
      positional encodings to be broadcast across all training examples later.
      Now we can visulize our positional encodings:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
pos_encoding = positional_encoding(50, 512)

print (pos_encoding.shape)

plt.pcolormesh(pos_encoding[0], cmap='RdBu')
plt.xlabel('d')
plt.xlim((0, 512))
plt.ylabel('Position')
plt.colorbar()
plt.show()</pre>
    </div>
    <p>
      =&gt;
    </p>
    <pre class="verbatim" xml:space="preserve">
(1,50,512)</pre>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-56.png" width="70%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 8. </b><a id="auto-21"></a>Visualization of Positional
              Encodings
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      Each row represents the positional encoding for the word at that
      specific position. From the visualization we can see that every row has
      a distinct color pattern, indicating that no two rows share identical
      encoding values. This property enables the positional encoding to
      preserve information about the absolute position of each word within the
      sequence. 
    </p>
    <p>
      Another interesting property is that the norm of the difference between
      two positional vectors separated by <tt class="verbatim">k</tt> positions remains
      constant. In other words, if we keep <tt class="verbatim">k</tt> fixed and change
      <tt class="verbatim">pos</tt>, the difference stays approximately the same. This
      shows that the encoding depends only on relative distances, which helps
      the model capture the relative positions of words.
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
pos1 = 42
pos2 = 77
k = 2

print(tf.norm(pos_encoding[0,pos1,:] - pos_encoding[0,pos1 + k,:]))
print(tf.norm(pos_encoding[0,pos2,:] - pos_encoding[0,pos2 + k,:]))</pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
tf.Tensor(3.2668781, shape=(), dtype=float32) 
tf.Tensor(3.2668781, shape=(), dtype=float32)</pre>
    </div>
    <p>
      We next implement two types of useful masks when building the
      Transformer archetecture. 
    </p>
  </body>
</html>