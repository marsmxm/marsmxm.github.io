<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:x="https://www.texmacs.org/2002/extensions" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <head>
    <title>mxm notes</title>
    <meta charset="utf-8" content="TeXmacs 2.1.4" name="generator"></meta>
    <link href="../resources/notes-base.css" type="text/css" rel="stylesheet"></link>
    <link href="../resources/blog-icon.png" rel="icon"></link>
    <script src="../resources/highlight.pack.js" language="javascript" defer></script>
    <script src="../resources/notes-base.js" language="javascript" defer></script>
  </head>
  <body>
    <div class="toggle" style="display: none">
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
    </div>
    <div class="notes-header">
      <p>
        <img class="image" src="../penguin.png" width="28.116784"></img><span style="margin-left: 2pt"></span><a href="./main.html">[main]</a><em
        class="notes-header-name">Notes on Programming and Others</em>
      </p>
    </div>
    <p>
      <a id="auto-1"></a>
    </p>
    <h1>Understanding Attention Mechanisms<span style="margin-left: 1em"></span></h1>
    <p>
      This article discusses the attention mechanisms both as an extension to
      the encoder-decoder RNN models, and as a fundamental component to the
      Transformer architecture. It serves as a record of my learning process
      and a reference for the future use.
    </p>
    <h2 id="auto-2">Encoder-Decoder Neural Network<span style="margin-left: 1em"></span></h2>
    <p>
      Machine translation can be viewed as constructing conditional language
      models that generate the most probable output in a target language for a
      given input in the other source language. To express this in a
      mathematical way:
    </p>
    <center>
      <img src="attention-1.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0112837465564738em; vertical-align: -1.08879338842975em; height: 2.01970247933884em"></img>
    </center>
    <p>
      Here, x and y denote the input and output sequence respectively, while
      superscripts in angle brackets (<span class="no-breaks"><img src="attention-2.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: 0em; vertical-align: -0.330953168044077em; height: 0.996716253443526em"></img>)</span>
      indicate the position of each word within the sequence. Traditionally, a
      method called statistical machine translation (SMT) was the dominant
      approach (as used in early versions of Google Translate). SMT models are
      trained on large amount of <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpra</a>, and analyzes
      these corpora to identify statistical relationships between words,
      phrases, and sentence structures in different languages.
    </p>
    <p>
      During the 2010s, another method called neural machine translation (NMT)
      rapidly gained popularity following the successful application of RNN
      models to translation tasks. A <a href="https://arxiv.org/abs/1409.3215">2014 paper</a> demonstrated how
      <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long-short-term memory (LSTM)</a> cells could be employed to
      address sequence-to-sequence problems. The idea is to use an encoder
      LSTM to read the input sequence, one timestep at a time, generating a
      fixed-dimensional vector representation, which was then decoded by a
      second LSTM to produce the corresponding output sequence.
    </p>
    <p>
      To better understand this neural network architecture, we will build an
      encoder&ndash;decoder network to solve a relatively simple task:
      converting human-readable date strings into the ISO date format. And
      here are some task samples:
    </p>
    <center>
      <img src="attention-3.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.259504132231405em; margin-right: -0.0112837465564724em; margin-top: -0.0112837465564737em; vertical-align: -2.06981818181818em; height: 4.37661707988981em"></img>
    </center>
    <p>
      The figure below describes our intented architecture:
    </p>
    <div style="margin-top: 1em; margin-bottom: 1em">
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img class="image" src="attention-4.png" width="100%"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><div class="caption">
            <font style="font-size: 90.0%"><p>
              <b>Figure 1. </b><a id="auto-3"></a>A encoder-decoder network
            </p></font>
          </div></td>
        </tr></tbody>
      </table>
    </div>
    <p>
      The left-side LSTM is the encoder. To better capture information from
      the input sequences, we use a bidirectional RNN for this component. The
      encoder's hidden and cell states form the vector representation of input
      sequence, and they are then passed to the decoder as its initial state.
      The ISO-formatted dates are also used as inputs to the decoder during
      training, but shifted back by one step. In other words, during training
      the decoder is given as input the character that it should have output
      at the previous step, regardless of what it actually output. This is
      called teacher forcing&mdash;a technique that improves the model's
      performance. The decoder returns all cells' output sequences instead of
      states from the last cell. It then passes its output to a dense layer
      which uses softmax as the activation funciton, to get probabilities of
      each output character. We will now build this network step by step (the
      complete implementation can be found <a href="https://github.com/marsmxm/marsmxm.github.io/blob/main/resources/articles/attention/seq2seq.py">here</a>).
    </p>
    <p>
      First we need to prepare the training dataset. The <a href="https://faker.readthedocs.io/en/master/">faker</a> is
      used here to generate some random dates, which are then formatted in
      random formats:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>def load_date():
    dt = fake.date_object()
    try:
        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US')
        human_readable = human_readable.lower()
        human_readable = human_readable.replace(',','')
        machine_readable = dt.isoformat()
        
    except AttributeError as e:
        return None, None, None

    return human_readable, machine_readable, dt</tt></class></pre>
    </div>
    <p>
      
    </p>
    <p>
      The size of our training dataset is 100,000:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>m = 100000
dataset = load_dataset(m)
dataset[:5]</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>[('saturday june 29 1996', '1996-06-29'),
 ('15 march 1978', '1978-03-15'),
 ('thursday december 28 2023', '2023-12-28'),
 ('wednesday december 31 1980', '1980-12-31'),
 ('apr 5 1995', '1995-04-05')]</tt></class></pre>
    </div>
    <p>
      
    </p>
    <p>
      The next step is to vectorize all the texts in the dataset. Since we are
      translating date strings, we will use character-level vectorization
      rather than word-level vectorization (as commonly used in NLP):
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>import tensorflow as tf

vocab_size = 50
Tx = 30
Ty = 12
sos = '@'
eos = '$'

def custom_standardization(input_string):
    # Lowercase and remove punctuation except '-'
    lowercase = tf.strings.lower(input_string)
    # Remove all punctuation except '-'
    return tf.strings.regex_replace(lowercase, r&quot;[^\w\s-@$]&quot;, &quot;&quot;)

dates_human = [d[0] for d in dataset]
dates_machine = [d[1] for d in dataset]

vec_layer_human = tf.keras.layers.TextVectorization(
    vocab_size, output_sequence_length=Tx, split=&quot;character&quot;, name=&quot;vec_h&quot;,
    standardize=custom_standardization)
vec_layer_machine = tf.keras.layers.TextVectorization(
    vocab_size, output_sequence_length=Ty, split=&quot;character&quot;, name=&quot;vec_m&quot;,
    standardize=custom_standardization)
    
vec_layer_human.adapt(dates_human)
vec_layer_machine.adapt([f&quot;{sos}{s}{eos}&quot; for s in dates_machine])

print(vec_layer_human.get_vocabulary()[:15])
print(vec_layer_machine.get_vocabulary())</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>['', '[UNK]', ' ', '1', '2', 'a', '0', '9', 'e', 'r', 'y', 'u', 'd', 's', 'n']
['', '[UNK]', '-', '0', '1', '2', '@', '$', '9', '7', '8', '3', '4', '5', '6']</tt></class></pre>
    </div>
    <p>
      The empty string (&ldquo;&rdquo;) and <tt class="verbatim"><class style="font-family: Times New Roman"><tt>[UNK]</tt></class></tt>
      are tensorflow's built-in representations for padding and unknown
      characters, repectively. We use a custom <tt class="verbatim">strandardization</tt>
      function here because we need two special characters, <tt class="verbatim">@</tt>
      and <tt class="verbatim">$</tt>, to denote the start and end of the sequences, and
      in the default settings, these special characters are removed by
      tensorflow's <tt class="verbatim">TextVectorization</tt>. 
    </p>
    <p>
      Next we split the whole dataset into training, validation and test sets:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>train_size = 60000
valid_size = 20000

X_train = tf.constant(dates_human[:train_size])
X_valid = tf.constant(dates_human[train_size:train_size+valid_size])
X_test = tf.constant(dates_human[train_size+valid_size:])

X_train_dec = tf.constant([f&quot;{sos}{s}&quot; for s in dates_machine[:train_size]])
X_valid_dec = tf.constant(
    [f&quot;{sos}{s}&quot; for s in dates_machine[train_size:train_size+valid_size]])
X_test_dec = tf.constant([f&quot;{sos}{s}&quot; for s in dates_machine[train_size+valid_size:]])

Y_train = vec_layer_machine([f&quot;{s}{eos}&quot; for s in dates_machine[:train_size]])
Y_valid = vec_layer_machine(
    [f&quot;{s}{eos}&quot; for s in dates_machine[train_size:train_size+valid_size]])
Y_test = vec_layer_machine([f&quot;{s}{eos}&quot; for s in dates_machine[train_size+valid_size:]])</tt></class></pre>
    </div>
    <p>
      
    </p>
    <p>
      And we can construct our model now:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>encoder_inputs = tf.keras.layers.Input(name=&quot;encoder_inputs&quot;, shape=[], dtype=tf.string)
decoder_inputs = tf.keras.layers.Input(name=&quot;decoder_inputs&quot;,shape=[], dtype=tf.string)

embed_size = 128
encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, name=&quot;enc_emb&quot;, mask_zero=True)
decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, name=&quot;dec_emb&quot;, mask_zero=True)

encoder_input_ids = vec_layer_human(encoder_inputs)
decoder_input_ids = vec_layer_machine(decoder_inputs)
encoder_embeddings = encoder_embedding_layer(encoder_input_ids)
decoder_embeddings = decoder_embedding_layer(decoder_input_ids)

encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_state=True), name=&quot;encoder&quot;)
encoder_outputs, *encoder_states = encoder(encoder_embeddings)
encoder_states = [tf.concat(encoder_states[::2], axis=-1),  # hidden states (0 &amp; 2)
                  tf.concat(encoder_states[1::2], axis=-1)] # cell states (1 &amp; 3)

decoder = tf.keras.layers.LSTM(512, name=&quot;decoder&quot;, return_sequences=True)
decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_states)

output_layer = tf.keras.layers.Dense(vocab_size, name=&quot;dense&quot;, activation=&quot;softmax&quot;)
Y_proba = output_layer(decoder_outputs)

model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],
                       outputs=[Y_proba])
model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;,
              metrics=[&quot;accuracy&quot;])</tt></class></pre>
    </div>
    <p>
      
    </p>
    <p>
      One thing worth mentioning is how we compose the <tt class="verbatim">encoder_states</tt>.
      Since we use a bidirectional LSTM as the encoder, we get four states
      from it in total. The first two are hidden and cell states of the
      forward LSTM, and the last two are the corresponding states from the
      backward one. The hidden states are concatenated together, and the cell
      states are concatenated likewise, before being passed to the decoder. We
      can have a look at the model summary now:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>model.summary(line_length=120, expand_nested=True)</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>________________________________________________________________________________________________________________________
 Layer (type)                       Output Shape                        Param #     Connected to                        
========================================================================================================================
 encoder_inputs (InputLayer)        [(None,)]                           0           []                                  
                                                                                                                        
 vec_h (TextVectorization)          (None, 30)                          0           ['encoder_inputs[0][0]']            
                                                                                                                        
 decoder_inputs (InputLayer)        [(None,)]                           0           []                                  
                                                                                                                        
 enc_emb (Embedding)                (None, 30, 128)                     6400        ['vec_h[0][0]']                     
                                                                                                                        
 vec_m (TextVectorization)          (None, 12)                          0           ['decoder_inputs[0][0]']            
                                                                                                                        
 encoder (Bidirectional)            [(None, 512),                       788480      ['enc_emb[0][0]']                   
                                     (None, 256),                                                                       
                                     (None, 256),                                                                       
                                     (None, 256),                                                                       
                                     (None, 256)]                                                                       
                                                                                                                        
 dec_emb (Embedding)                (None, 12, 128)                     6400        ['vec_m[0][0]']                     
                                                                                                                        
 tf.concat (TFOpLambda)             (None, 512)                         0           ['encoder[0][1]',                   
                                                                                     'encoder[0][3]']                   
                                                                                                                        
 tf.concat_1 (TFOpLambda)           (None, 512)                         0           ['encoder[0][2]',                   
                                                                                     'encoder[0][4]']                   
                                                                                                                        
 decoder (LSTM)                     (None, 12, 512)                     1312768     ['dec_emb[0][0]',                   
                                                                                     'tf.concat[0][0]',                 
                                                                                     'tf.concat_1[0][0]']               
                                                                                                                        
 dense (Dense)                      (None, 12, 50)                      25650       ['decoder[0][0]']                   
                                                                                                                        
========================================================================================================================
Total params: 2139698 (8.16 MB)
Trainable params: 2139698 (8.16 MB)
Non-trainable params: 0 (0.00 Byte)</tt></class></pre>
    </div>
    <p>
      
    </p>
    <p>
      It indeed has the same topology as we showed earlier in figure 1. The
      training process taks around 2 to 3 minutes on my RTX 3060 card, and the
      evalution gets a promising result:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>model.fit((X_train, X_train_dec), Y_train, epochs=10,
          validation_data=((X_valid, X_valid_dec), Y_valid))

print(&quot;Evaluate on test data&quot;)
results = model.evaluate((X_test, X_test_dec), Y_test)
print(&quot;test loss, test acc:&quot;, results)</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>test loss, test acc: [0.0018235821044072509, 0.9993454813957214]</tt></class></pre>
    </div>
    <p>
      We can use our model to convert date formats finally, but it's not as
      simple as calling <tt class="verbatim"><class style="font-family: Times New Roman"><tt>model.predict()</tt></class></tt>:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>import numpy as np

def translate(hunman_date):
    translation = &quot;&quot;
    for t in range(Ty):
        X = np.array([hunman_date])  # encoder input 
        X_dec = np.array([sos + translation])  # decoder input
        y_proba = model.predict((X, X_dec), verbose=0)[0, t]  # last token's probas
        char_id = np.argmax(y_proba)
        predicted_char = vec_layer_machine.get_vocabulary()[char_id]
        if predicted_char == eos:
            break
        translation += predicted_char
    return translation.strip()</tt></class></pre>
    </div>
    <p>
      
    </p>
    <p>
      We call the model in a loop because the decoder expects as input the
      character that was predicted at the previous time step. Let's see how
      our model performs:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>for i in range(5):
    human_date= format_date(fake.date_object(),format=random.choice(FORMATS),locale='en_US')
    print(&quot;human: &quot; + human_date)
    print(&quot;machine: &quot; + translate(human_date) + &quot;\n&quot;)</tt></class></pre>
    </div>
    <p>
      =&gt;
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>human: Friday, December 20, 2019
machine: 2019-12-20

human: 14.02.73
machine: 1973-02-14

human: Jan 10, 2025
machine: 2025-01-10

human: Friday, February 22, 1985
machine: 1985-02-22

human: Sunday, September 12, 1971
machine: 1971-09-12</tt></class></pre>
    </div>
    <p>
      It did really well.
    </p>
    <h2 id="auto-4">Attention Mechanisms<span style="margin-left: 1em"></span></h2>
  </body>
</html>