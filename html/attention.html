<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:x="https://www.texmacs.org/2002/extensions" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <head>
    <title>mxm notes</title>
    <meta charset="utf-8" content="TeXmacs 2.1.4" name="generator"></meta>
    <link href="../resources/notes-base.css" type="text/css" rel="stylesheet"></link>
    <link href="../resources/blog-icon.png" rel="icon"></link>
    <script src="../resources/highlight.pack.js" language="javascript" defer></script>
    <script src="../resources/notes-base.js" language="javascript" defer></script>
  </head>
  <body>
    <div class="toggle" style="display: none">
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
    </div>
    <div class="notes-header">
      <p>
        <img class="image" src="../penguin.png" width="28.116784"></img><span style="margin-left: 2pt"></span><a href="./main.html">[main]</a><em
        class="notes-header-name">Notes on Programming and Others</em>
      </p>
    </div>
    <p>
      <a id="auto-1"></a>
    </p>
    <h1>Understanding Attention Mechanisms<span style="margin-left: 1em"></span></h1>
    <p>
      This article discusses the attention mechanisms both as an extension to
      the encoder-decoder RNN models, and as a fundamental component to the
      Transformer architecture. It serves as a record of my learning process
      and a reference for the future use.
    </p>
    <h2 id="auto-2">Encoder-Decoder Neural Network<span style="margin-left: 1em"></span></h2>
    <p>
      Machine translation can be viewed as constructing conditional language
      models that generate the most probable output in a target language for a
      given input in the other source language. To express this in a
      mathematical way:
    </p>
    <center>
      <img src="attention-1.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564741em; margin-top: -0.0112837465564738em; vertical-align: -1.08879338842975em; height: 2.01970247933884em"></img>
    </center>
    <p>
      Here, x and y denote the input and output sequence respectively, while
      superscripts in angle brackets (<span class="no-breaks"><img src="attention-2.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.0112617079889807em; margin-right: -0.0112837465564732em; margin-top: 0em; vertical-align: -0.330953168044077em; height: 0.996716253443526em"></img>)</span>
      indicate the position of each word within the sequence. Traditionally, a
      method called statistical machine translation (SMT) was the dominant
      approach (as used in early versions of Google Translate). SMT models are
      trained on large amount of <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpra</a>, and analyzes
      these corpora to identify statistical relationships between words,
      phrases, and sentence structures in different languages.
    </p>
    <p>
      During the 2010s, another method called neural machine translation (NMT)
      rapidly gained popularity following the successful application of RNN
      models to translation tasks. A <a href="https://arxiv.org/abs/1409.3215">2014 paper</a> demonstrated how
      <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long-short-term memory (LSTM)</a> cells could be employed to
      address sequence-to-sequence problems. The idea is to use an encoder
      LSTM to read the input sequence, one timestep at a time, generating a
      fixed-dimensional vector representation, which was then decoded by a
      second LSTM to produce the corresponding output sequence.
    </p>
    <p>
      To better understand this architecture, we will build an
      encoder&ndash;decoder network to solve a relatively simple task:
      converting human-readable date strings into the ISO date format. The
      complete code for this part can be found <a href="https://github.com/marsmxm/marsmxm.github.io/blob/main/resources/articles/attention/seq2seq.py">here</a>. 
    </p>
    <p>
      So here are some examples of our translation task:
    </p>
    <center>
      <img src="attention-3.png" style="margin-left: -0.0112837465564738em; margin-bottom: 0.259504132231405em; margin-right: -0.0112837465564724em; margin-top: -0.0112837465564737em; vertical-align: -1.46263360881543em; height: 3.1622258953168em"></img>
    </center>
    <p>
      First we need to prepare the training dataset. We use <a href="https://faker.readthedocs.io/en/master/">faker</a>
      to generate random dates, and format them in random formats:
    </p>
    <div class="tmweb-code">
      <pre class="verbatim" xml:space="preserve">
<class style="font-family: Times New Roman"><tt>def load_date():
    dt = fake.date_object()
    try:
        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US')
        human_readable = human_readable.lower()
        human_readable = human_readable.replace(',','')
        machine_readable = dt.isoformat()
        
    except AttributeError as e:
        return None, None, None

    return human_readable, machine_readable, dt</tt></class></pre>
    </div>
    <p>
      
    </p>
    <p>
      These are some samples from our dataset:
    </p>
    <div class="tmweb-code">
      <p>
        [('saturday june 29 1996', '1996-06-29'),  
      </p>
      <p>
        ('15 march 1978', '1978-03-15'),  
      </p>
      <p>
        ('thursday december 28 2023', '2023-12-28'),  
      </p>
      <p>
        ('wednesday december 31 1980', '1980-12-31'),  
      </p>
      <p>
        ('apr 5 1995', '1995-04-05')]
      </p>
    </div>
    <p>
      
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
  </body>
</html>
